{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "데이터 로드 및 사전 처리 #5 - Text",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kfiyXXm9jMr"
      },
      "source": [
        "# 텍스트 로드하기\n",
        "\n",
        "이번 튜토리얼에서는 텍스트를 로드하고 전처리하는 두 가지 방법을 소개합니다.\n",
        "+ keras 유틸리티와 레이어 사용하기\n",
        "+ 텍스트 파일을 로드하기 위해 `tf.data.TextLineDataset`를 사용하고, 데이터를 전처리하기 위해 `tf.text`를 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlFNuUlL9HrV",
        "outputId": "e51434b9-cd3d-4b96-f9b3-b2b0fed78acd"
      },
      "source": [
        "!pip uninstall -y tensorflow tf-nightly keras\n",
        "\n",
        "!pip install -q -U tf-nightly\n",
        "!pip install -q -U tensorflow-text-nightly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "Uninstalling tf-nightly-2.7.0.dev20210713:\n",
            "  Successfully uninstalled tf-nightly-2.7.0.dev20210713\n",
            "\u001b[33mWARNING: Skipping keras as it is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FqyPQCl-WEh"
      },
      "source": [
        "import collections, pathlib, re, string\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvT7k5Zc-9Zf"
      },
      "source": [
        "## 1. 스택 오버플로우를 위한 태그 예측하기\n",
        "\n",
        "스택 오버플로우로부터 프로그래밍 질문 데이터세트를 다운로드합니다. 각 질문(어떻게 딕셔너리를 값에 따라 정렬하는가?)은 하나의 태그(`Python`, `CSharp`, `JavaScript`, `Java` 중 하나)에 레이블되어 있습니다.\n",
        "\n",
        "이번 과제는 질문에 대한 태그를 예측하는 모델을 개발하는 멀티 클래스 분류 작업입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klwQGFawAFKf"
      },
      "source": [
        "### 데이터세트 다운로드 및 살펴보기\n",
        "\n",
        "이제 데이터세트를 다운로드하고 딕셔너리 구조를 살펴봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtHsXlnvANCy"
      },
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "dataset_dir = tf.keras.utils.get_file(\n",
        "    origin=data_url,\n",
        "    untar=True,\n",
        "    cache_dir='stack_overflow',\n",
        "    cache_subdir=''\n",
        ")\n",
        "\n",
        "# 경로를 PosixPath로 나타내도록 한다. .parent는 가장 마지막 파일 바로 위까지의 경로를 나타낸다.\n",
        "dataset_dir = pathlib.Path(dataset_dir).parent"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll3bLJ5gBuB1",
        "outputId": "2118492e-bca4-4383-9006-cd065c639158"
      },
      "source": [
        "list(dataset_dir.iterdir())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/tmp/.keras/test'),\n",
              " PosixPath('/tmp/.keras/stack_overflow_16k.tar.gz'),\n",
              " PosixPath('/tmp/.keras/train'),\n",
              " PosixPath('/tmp/.keras/README.md')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhEwUcr6ByOR",
        "outputId": "221ab940-49eb-4ab4-83df-7723dba9b585"
      },
      "source": [
        "train_dir = dataset_dir/'train'\n",
        "list(train_dir.iterdir())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/tmp/.keras/train/python'),\n",
              " PosixPath('/tmp/.keras/train/javascript'),\n",
              " PosixPath('/tmp/.keras/train/csharp'),\n",
              " PosixPath('/tmp/.keras/train/java')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NpbX7UFEJlq"
      },
      "source": [
        "`train/csharp`, `train/java`, `train/python`, `train/javascript` 디렉토리에는 많은 텍스트 파일들이 저장되어 있습니다. 각 파일들은 스택 오버플로우 질문들입니다. 파일 하나의 데이터를 살펴봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFovTod4B-KD",
        "outputId": "de34aa4a-34a3-4a81-e845-8101c14a1b6b"
      },
      "source": [
        "sample_file = train_dir/'python/1755.txt'\n",
        "print(open(sample_file).read())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "why does this blank program print true x=true.def stupid():.    x=false.stupid().print x\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w6Sfke8E33v"
      },
      "source": [
        "### 데이터세트 로드 및 구성하기\n",
        "\n",
        "이제 디스크에서 데이터를 로드하고 훈련에 적합한 포맷으로 만들 것입니다. `tf.data.Dataset`을 생성하기 위해 `text_dataset_from_directory`를 사용하겠습니다. `tf.data`는 입력 파이프라인을 빌드하는데 아주 강력한 툴입니다.\n",
        "\n",
        "`preprocessing.text_dataset_from_directory`는 아래와 같은 디렉토리 구조를 가집니다.\n",
        "\n",
        "```\n",
        "train/\n",
        "...csharp/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...java/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...javascript/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...python/\n",
        "......1.txt\n",
        "......2.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPxvVKGuG-Qe"
      },
      "source": [
        "머신 러닝 실험을 진행할 때, 데이터세트를 훈련, 검증, 테스트의 3가지로 나누는 것이 가장 좋습니다. 스택 오버플로우 데이터세트는 이미 훈련과 테스트가 나뉘어져 있으므로, 훈련 데이터세트에서 8:2 비율로 검증 세트를 하나 만들겠습니다. `validation_split`을 이용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJOglIaPEoWq",
        "outputId": "c23f92a1-1cd8-442d-ba9a-cffd377f9992"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = preprocessing.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size = batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 6400 files for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S_xnClnHoAk"
      },
      "source": [
        "위에서 보았듯, 8,000개의 샘플들이 훈련 폴더 내에 있고 그 중 80%인 6,400개가 훈련 데이터세트가 된 것을 확인할 수 있습니다. 6,400개의 데이터는 각각 32개씩 배치(데이터 묶음)을 이루고 있습니다.\n",
        "\n",
        "훈련 데이터세트는 이후에 `tf.data.Dataset`을 거쳐 `model.fit`으로 들어가 학습하는 용도로 쓰이게 됩니다.\n",
        "\n",
        "훈련 데이터세트의 example과 label을 살펴봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoCGfAPGHm8c",
        "outputId": "a4302259-f76b-40d9-8c82-bd3d9829a7c8"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(10):\n",
        "        print(\"Question: \", text_batch.numpy()[i])\n",
        "        print('Label: ', label_batch.numpy()[i])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:  b'\"my tester is going to the wrong constructor i am new to programming so if i ask a question that can be easily fixed, please forgive me. my program has a tester class with a main. when i send that to my regularpolygon class, it sends it to the wrong constructor. i have two constructors. 1 without perameters..public regularpolygon().    {.       mynumsides = 5;.       mysidelength = 30;.    }//end default constructor...and my second, with perameters. ..public regularpolygon(int numsides, double sidelength).    {.        mynumsides = numsides;.        mysidelength = sidelength;.    }// end constructor...in my tester class i have these two lines:..regularpolygon shape = new regularpolygon(numsides, sidelength);.        shape.menu();...numsides and sidelength were declared and initialized earlier in the testing class...so what i want to happen, is the tester class sends numsides and sidelength to the second constructor and use it in that class. but it only uses the default constructor, which therefor ruins the whole rest of the program. can somebody help me?..for those of you who want to see more of my code: here you go..public double vertexangle().    {.        system.out.println(\"\"the vertex angle method: \"\" + mynumsides);// prints out 5.        system.out.println(\"\"the vertex angle method: \"\" + mysidelength); // prints out 30..        double vertexangle;.        vertexangle = ((mynumsides - 2.0) / mynumsides) * 180.0;.        return vertexangle;.    }//end method vertexangle..public void menu().{.    system.out.println(mynumsides); // prints out what the user puts in.    system.out.println(mysidelength); // prints out what the user puts in.    gotographic();.    calcr(mynumsides, mysidelength);.    calcr(mynumsides, mysidelength);.    print(); .}// end menu...this is my entire tester class:..public static void main(string[] arg).{.    int numsides;.    double sidelength;.    scanner keyboard = new scanner(system.in);..    system.out.println(\"\"welcome to the regular polygon program!\"\");.    system.out.println();..    system.out.print(\"\"enter the number of sides of the polygon ==&gt; \"\");.    numsides = keyboard.nextint();.    system.out.println();..    system.out.print(\"\"enter the side length of each side ==&gt; \"\");.    sidelength = keyboard.nextdouble();.    system.out.println();..    regularpolygon shape = new regularpolygon(numsides, sidelength);.    shape.menu();.}//end main...for testing it i sent it numsides 4 and sidelength 100.\"\\n'\n",
            "Label:  1\n",
            "Question:  b'\"blank code slow skin detection this code changes the color space to lab and using a threshold finds the skin area of an image. but it\\'s ridiculously slow. i don\\'t know how to make it faster ?    ..from colormath.color_objects import *..def skindetection(img, treshold=80, color=[255,20,147]):..    print img.shape.    res=img.copy().    for x in range(img.shape[0]):.        for y in range(img.shape[1]):.            rgbimg=rgbcolor(img[x,y,0],img[x,y,1],img[x,y,2]).            labimg=rgbimg.convert_to(\\'lab\\', debug=false).            if (labimg.lab_l &gt; treshold):.                res[x,y,:]=color.            else: .                res[x,y,:]=img[x,y,:]..    return res\"\\n'\n",
            "Label:  3\n",
            "Question:  b'\"option and validation in blank i want to add a new option on my system where i want to add two text files, both rental.txt and customer.txt. inside each text are id numbers of the customer, the videotape they need and the price...i want to place it as an option on my code. right now i have:...add customer.rent return.view list.search.exit...i want to add this as my sixth option. say for example i ordered a video, it would display the price and would let me confirm the price and if i am going to buy it or not...here is my current code:..  import blank.io.*;.    import blank.util.arraylist;.    import static blank.lang.system.out;..    public class rentalsystem{.    static bufferedreader input = new bufferedreader(new inputstreamreader(system.in));.    static file file = new file(\"\"file.txt\"\");.    static arraylist&lt;string&gt; list = new arraylist&lt;string&gt;();.    static int rows;..    public static void main(string[] args) throws exception{.        introduction();.        system.out.print(\"\"nn\"\");.        login();.        system.out.print(\"\"nnnnnnnnnnnnnnnnnnnnnn\"\");.        introduction();.        string repeat;.        do{.            loadfile();.            system.out.print(\"\"nwhat do you want to do?nn\"\");.            system.out.print(\"\"n                    - - - - - - - - - - - - - - - - - - - - - - -\"\");.            system.out.print(\"\"nn                    |     1. add customer    |   2. rent return |n\"\");.            system.out.print(\"\"n                    - - - - - - - - - - - - - - - - - - - - - - -\"\");.            system.out.print(\"\"nn                    |     3. view list       |   4. search      |n\"\");.            system.out.print(\"\"n                    - - - - - - - - - - - - - - - - - - - - - - -\"\");.            system.out.print(\"\"nn                                             |   5. exit        |n\"\");.            system.out.print(\"\"n                                              - - - - - - - - - -\"\");.            system.out.print(\"\"nnchoice:\"\");.            int choice = integer.parseint(input.readline());.            switch(choice){.                case 1:.                    writedata();.                    break;.                case 2:.                    rentdata();.                    break;.                case 3:.                    viewlist();.                    break;.                case 4:.                    search();.                    break;.                case 5:.                    system.out.println(\"\"goodbye!\"\");.                    system.exit(0);.                default:.                    system.out.print(\"\"invalid choice: \"\");.                    break;.            }.            system.out.print(\"\"ndo another task? [y/n] \"\");.            repeat = input.readline();.        }while(repeat.equals(\"\"y\"\"));..        if(repeat!=\"\"y\"\") system.out.println(\"\"ngoodbye!\"\");..    }..    public static void writedata() throws exception{.        system.out.print(\"\"nname: \"\");.        string cname = input.readline();.        system.out.print(\"\"address: \"\");.        string add = input.readline();.        system.out.print(\"\"phone no.: \"\");.        string pno = input.readline();.        system.out.print(\"\"rental amount: \"\");.        string ramount = input.readline();.        system.out.print(\"\"tapenumber: \"\");.        string tno = input.readline();.        system.out.print(\"\"title: \"\");.        string title = input.readline();.        system.out.print(\"\"date borrowed: \"\");.        string dborrowed = input.readline();.        system.out.print(\"\"due date: \"\");.        string ddate = input.readline();.        createline(cname, add, pno, ramount,tno, title, dborrowed, ddate);.        rentdata();.    }..    public static void createline(string name, string address, string phone , string rental, string tapenumber, string title, string borrowed, string due) throws exception{.        filewriter fw = new filewriter(file, true);.        fw.write(\"\"nname: \"\"+name + \"\"naddress: \"\" + address +\"\"nphone no.: \"\"+ phone+\"\"nrentalamount: \"\"+rental+\"\"ntape no.: \"\"+ tapenumber+\"\"ntitle: \"\"+ title+\"\"ndate borrowed: \"\"+borrowed +\"\"ndue date: \"\"+ due+\"\":rn\"\");.        fw.close();.    }..    public static void loadfile() throws exception{.        try{.            list.clear();.            fileinputstream fstream = new fileinputstream(file);.            bufferedreader br = new bufferedreader(new inputstreamreader(fstream));.            rows = 0;.            while( br.ready()).            {.                list.add(br.readline());.                rows++;.            }.            br.close();.        } catch(exception e){.            system.out.println(\"\"list not yet loaded.\"\");.        }.    }..    public static void viewlist(){.        system.out.print(\"\"n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\" |list of all costumers|\"\");.        system.out.print(\"\"~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        for(int i = 0; i &lt;rows; i++){.            system.out.println(list.get(i));.        }.    }.        public static void rentdata()throws exception.    {   system.out.print(\"\"n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\" |rent data list|\"\");.        system.out.print(\"\"~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\"nenter customer name: \"\");.        string cname = input.readline();.        system.out.print(\"\"date borrowed: \"\");.        string dborrowed = input.readline();.        system.out.print(\"\"due date: \"\");.        string ddate = input.readline();.        system.out.print(\"\"return date: \"\");.        string rdate = input.readline();.        system.out.print(\"\"rent amount: \"\");.        string ramount = input.readline();..        system.out.print(\"\"you pay:\"\"+ramount);...    }.    public static void search()throws exception.    {   system.out.print(\"\"n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\" |search costumers|\"\");.        system.out.print(\"\"~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\"nenter costumer name: \"\");.        string cname = input.readline();.        boolean found = false;..        for(int i=0; i &lt; rows; i++){.            string temp[] = list.get(i).split(\"\",\"\");..            if(cname.equals(temp[0])){.            system.out.println(\"\"search result:nyou are \"\" + temp[0] + \"\" from \"\" + temp[1] + \"\".\"\"+ temp[2] + \"\".\"\"+ temp[3] + \"\".\"\"+ temp[4] + \"\".\"\"+ temp[5] + \"\" is \"\" + temp[6] + \"\".\"\"+ temp[7] + \"\" is \"\" + temp[8] + \"\".\"\");.                found = true;.            }.        }..        if(!found){.            system.out.print(\"\"no results.\"\");.        }..    }..        public static boolean evaluate(string uname, string pass){.        if (uname.equals(\"\"admin\"\")&amp;&amp;pass.equals(\"\"12345\"\")) return true;.        else return false;.    }..    public static string login()throws exception{.        bufferedreader input=new bufferedreader(new inputstreamreader(system.in));.        int counter=0;.        do{.            system.out.print(\"\"username:\"\");.            string uname =input.readline();.            system.out.print(\"\"password:\"\");.            string pass =input.readline();..            boolean accept= evaluate(uname,pass);..            if(accept){.                break;.                }else{.                    system.out.println(\"\"incorrect username or password!\"\");.                    counter ++;.                    }.        }while(counter&lt;3);..            if(counter !=3) return \"\"login successful\"\";.            else return \"\"login failed\"\";.            }.        public static void introduction() throws exception{..        system.out.println(\"\"                  - - - - - - - - - - - - - - - - - - - - - - - - -\"\");.        system.out.println(\"\"                  !                  r e n t a l                  !\"\");.        system.out.println(\"\"                   ! ~ ~ ~ ~ ~ !  =================  ! ~ ~ ~ ~ ~ !\"\");.        system.out.println(\"\"                  !                  s y s t e m                  !\"\");.        system.out.println(\"\"                  - - - - - - - - - - - - - - - - - - - - - - - - -\"\");.        }..}\"\\n'\n",
            "Label:  1\n",
            "Question:  b'\"exception: dynamic sql generation for the updatecommand is not supported against a selectcommand that does not return any key i dont know what is the problem this my code : ..string nomtable;..datatable listeetablissementtable = new datatable();.datatable listeinteretstable = new datatable();.dataset ds = new dataset();.sqldataadapter da;.sqlcommandbuilder cmdb;..private void listeinterets_click(object sender, eventargs e).{.    nomtable = \"\"listeinteretstable\"\";.    d.cnx.open();.    da = new sqldataadapter(\"\"select nome from offices\"\", d.cnx);.    ds = new dataset();.    da.fill(ds, nomtable);.    datagridview1.datasource = ds.tables[nomtable];.}..private void sauvgarder_click(object sender, eventargs e).{.    d.cnx.open();.    cmdb = new sqlcommandbuilder(da);.    da.update(ds, nomtable);.    d.cnx.close();.}\"\\n'\n",
            "Label:  0\n",
            "Question:  b'\"parameter with question mark and super in blank, i\\'ve come across a method that is formatted like this:..public final subscription subscribe(final action1&lt;? super t&gt; onnext, final action1&lt;throwable&gt; onerror) {.}...in the first parameter, what does the question mark and super mean?\"\\n'\n",
            "Label:  1\n",
            "Question:  b'call two objects wsdl the first time i got a very strange wsdl. ..i would like to call the object (interface - invoicecheck_out) do you know how?....i would like to call the object (variable) do you know how?..try to call (it`s ok)....try to call (how call this?)\\n'\n",
            "Label:  0\n",
            "Question:  b\"how to correctly make the icon for systemtray in blank using icon sizes of any dimension for systemtray doesn't look good overall. .what is the correct way of making icons for windows system tray?..screenshots: http://imgur.com/zsibwn9..icon: http://imgur.com/vsh4zo8\\n\"\n",
            "Label:  0\n",
            "Question:  b'\"is there a way to check a variable that exists in a different script than the original one? i\\'m trying to check if a variable, which was previously set to true in 2.py in 1.py, as 1.py is only supposed to continue if the variable is true...2.py..import os..completed = false..#some stuff here..completed = true...1.py..import 2 ..if completed == true.   #do things...however i get a syntax error at ..if completed == true\"\\n'\n",
            "Label:  3\n",
            "Question:  b'\"blank control flow i made a number which asks for 2 numbers with blank and responds with  the corresponding message for the case. how come it doesnt work  for the second number ? .regardless what i enter for the second number , i am getting the message \"\"your number is in the range 0-10\"\"...using system;.using system.collections.generic;.using system.linq;.using system.text;..namespace consoleapplication1.{.    class program.    {.        static void main(string[] args).        {.            string myinput;  // declaring the type of the variables.            int myint;..            string number1;.            int number;...            console.writeline(\"\"enter a number\"\");.            myinput = console.readline(); //muyinput is a string  which is entry input.            myint = int32.parse(myinput); // myint converts the string into an integer..            if (myint &gt; 0).                console.writeline(\"\"your number {0} is greater than zero.\"\", myint);.            else if (myint &lt; 0).                console.writeline(\"\"your number {0} is  less  than zero.\"\", myint);.            else.                console.writeline(\"\"your number {0} is equal zero.\"\", myint);..            console.writeline(\"\"enter another number\"\");.            number1 = console.readline(); .            number = int32.parse(myinput); ..            if (number &lt; 0 || number == 0).                console.writeline(\"\"your number {0} is  less  than zero or equal zero.\"\", number);.            else if (number &gt; 0 &amp;&amp; number &lt;= 10).                console.writeline(\"\"your number {0} is  in the range from 0 to 10.\"\", number);.            else.                console.writeline(\"\"your number {0} is greater than 10.\"\", number);..            console.writeline(\"\"enter another number\"\");..        }.    }    .}\"\\n'\n",
            "Label:  0\n",
            "Question:  b'\"credentials cannot be used for ntlm authentication i am getting org.apache.commons.httpclient.auth.invalidcredentialsexception: credentials cannot be used for ntlm authentication: exception in eclipse..whether it is possible mention eclipse to take system proxy settings directly?..public class httpgetproxy {.    private static final string proxy_host = \"\"proxy.****.com\"\";.    private static final int proxy_port = 6050;..    public static void main(string[] args) {.        httpclient client = new httpclient();.        httpmethod method = new getmethod(\"\"https://kodeblank.org\"\");..        hostconfiguration config = client.gethostconfiguration();.        config.setproxy(proxy_host, proxy_port);..        string username = \"\"*****\"\";.        string password = \"\"*****\"\";.        credentials credentials = new usernamepasswordcredentials(username, password);.        authscope authscope = new authscope(proxy_host, proxy_port);..        client.getstate().setproxycredentials(authscope, credentials);..        try {.            client.executemethod(method);..            if (method.getstatuscode() == httpstatus.sc_ok) {.                string response = method.getresponsebodyasstring();.                system.out.println(\"\"response = \"\" + response);.            }.        } catch (ioexception e) {.            e.printstacktrace();.        } finally {.            method.releaseconnection();.        }.    }.}...exception:...  dec 08, 2017 1:41:39 pm .          org.apache.commons.httpclient.auth.authchallengeprocessor selectauthscheme.         info: ntlm authentication scheme selected.       dec 08, 2017 1:41:39 pm org.apache.commons.httpclient.httpmethoddirector executeconnect.         severe: credentials cannot be used for ntlm authentication: .           org.apache.commons.httpclient.usernamepasswordcredentials.           org.apache.commons.httpclient.auth.invalidcredentialsexception: credentials .         cannot be used for ntlm authentication: .        enter code here .          org.apache.commons.httpclient.usernamepasswordcredentials.      at org.apache.commons.httpclient.auth.ntlmscheme.authenticate(ntlmscheme.blank:332).        at org.apache.commons.httpclient.httpmethoddirector.authenticateproxy(httpmethoddirector.blank:320).      at org.apache.commons.httpclient.httpmethoddirector.executeconnect(httpmethoddirector.blank:491).      at org.apache.commons.httpclient.httpmethoddirector.executewithretry(httpmethoddirector.blank:391).      at org.apache.commons.httpclient.httpmethoddirector.executemethod(httpmethoddirector.blank:171).      at org.apache.commons.httpclient.httpclient.executemethod(httpclient.blank:397).      at org.apache.commons.httpclient.httpclient.executemethod(httpclient.blank:323).      at httpgetproxy.main(httpgetproxy.blank:31).  dec 08, 2017 1:41:39 pm org.apache.commons.httpclient.httpmethoddirector processproxyauthchallenge.  info: failure authenticating with ntlm @proxy.****.com:6050\"\\n'\n",
            "Label:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5X4Ux6bIbs4"
      },
      "source": [
        "첫 번째 배치의 32개의 데이터 중 10개의 데이터를 출력해보았습니다. \n",
        "\n",
        "레이블이 0, 1, 2, 3으로 나타나는데 각 레이블이 의미하는 언어를 알아보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "engcVqXJIX6f",
        "outputId": "700e1f06-1ca2-4a72-8338-d76f2766be53"
      },
      "source": [
        "for i, label in enumerate(raw_train_ds.class_names):\n",
        "    print(\"Label\", i, \"corresponds to\", label)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label 0 corresponds to csharp\n",
            "Label 1 corresponds to java\n",
            "Label 2 corresponds to javascript\n",
            "Label 3 corresponds to python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a856jjvAJDjA"
      },
      "source": [
        "이제 검증 데이터세트를 만들어보겠습니다.\n",
        "\n",
        "주의: `validation_split`과 `subset`을 사용할 때 반드시 랜덤 seed를 명시하거나 `shuffle=False`을 적어주어, 훈련 데이터세트와 검증 데이터세트가 서로 겹치지 않도록 해야합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOmSAk_jI-cc",
        "outputId": "ce06b52b-3fde-42a0-9d5f-3300f548a752"
      },
      "source": [
        "raw_val_ds = preprocessing.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size = batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 1600 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhR79GOfJ3Kj"
      },
      "source": [
        "마지막으로 훈련 데이터세트를 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfw02g68JpR-",
        "outputId": "b88fbfe4-f011-4e25-e0ca-e62b826bc93a"
      },
      "source": [
        "test_dir = dataset_dir/'test'\n",
        "raw_test_ds = preprocessing.text_dataset_from_directory(\n",
        "    test_dir,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-hTKO31J5iK"
      },
      "source": [
        "### 훈련을 위한 데이터세트 준비하기(전처리 작업)\n",
        "\n",
        "이제 `preprocessing.TextVectorization` 레이어에 사용할 데이터를 표준화, 토큰화, 벡터화하는 전처리 작업을 거쳐야합니다.\n",
        "\n",
        "+ 표준화(Standardization) : 구두점 또는 HTML의 요소들을 제거해서 데이터세트를 단순화하는 텍스트 전처리 과정.\n",
        "+ 토큰화(Tokenization) : 문자열을 토큰으로 나누는 과정(예를 들어, 문장을 띄어쓰기에 따라 나눠서 단어로 만드는 과정이 이에 속합니다)\n",
        "+ 벡터화(Vectorization) : 나누어진 토큰들을 숫자로 변환하는 과정입니다. 벡터화된 숫자들이 신경망에 input으로 입력됩니다.\n",
        "\n",
        "각 과정들은 아래와 같이 레이어됩니다.\n",
        "\n",
        "+ 디폴트 표준화 작업은 텍스트를 소문자로 변환하고 구두점을 제거합니다.\n",
        "+ 디폴트 토큰화 작업은 공백에 따라 문장을 분리합니다.\n",
        "+ 디폴트 벡터화 모드는 `int`입니다. 각 정수는 하나의 단어를 가리킵니다. `binary`라는 다른 모드를 사용할 수도 있습니다. 이 모드는 bag-of-word 모델을 빌드하기 위함입니다.   \n",
        "\n",
        "벡터화 모드의 `binary`와 `int` 두 가지 모드를 모두 배워보겠습니다. 먼저 `binary` 모드를 이용해 bag-of-words 모델을 빌드해본 뒤에 `int` 모드를 `1D ConvNet`과 함께 사용해보겠습니다.\n",
        "\n",
        "먼저 각 `binary`와 `int` 벡터화 레이어를 만든 뒤에 `adapt`를 이용해 훈련 될 수 있도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMfC6Me-JyQx"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "# binary layer\n",
        "binary_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='binary'\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qg47fNbS9ZT"
      },
      "source": [
        "`int` 모드에서는 maximum vocabulary size와 더불어 maximum sequence length도 지정해주어야 합니다. maximum length를 벗어나는 문장의 경우 앞 또는 뒤에서부터 문장을 자릅니다. maximum length보다 부족한 길이일 경우 부족한만큼 0으로 채웁니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_p65CvnUIm3"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# int layer\n",
        "int_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KckhEPfpUbGw"
      },
      "source": [
        "다음으로 `adapt`를 콜해서 전처리 레이어의 상태를 데이터세트를 이용해 훈련시켜야합니다. 이 과정은 모델이 문자에 해당하는 정수를 빌드하도록 즉, 벡터화 해줍니다.\n",
        "\n",
        "주의: `adapt`를 콜할 때 반드시 훈련 데이터만을 사용해야합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx2jFLDTUTnQ"
      },
      "source": [
        "train_text = raw_train_ds.map(lambda text, label: text)\n",
        "binary_vectorize_layer.adapt(train_text)\n",
        "int_vectorize_layer.adapt(train_text)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6QDo5poWp1Z"
      },
      "source": [
        "데이터를 전처리하기 위해 이 레이어들을 이용한 결과를 살펴봅시다.\n",
        "\n",
        "[tf.expand_dims](https://www.tensorflow.org/api_docs/python/tf/expand_dims)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC34WTudW1tJ"
      },
      "source": [
        "def binary_vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)  # text의 마지막에 1을 추가한다.\n",
        "    return binary_vectorize_layer(text), label"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnsfBLx4XTSY"
      },
      "source": [
        "def int_vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return int_vectorize_layer(text), label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Bv8ZgDXaXT",
        "outputId": "529b6bf7-f940-4ca7-cab8-0d7af3469919"
      },
      "source": [
        "# 하나의 배치(32개의 데이터)를 데이터세트로부터 뽑아냅니다.\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_question, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Question: \", first_question)\n",
        "print('Label: ', first_label)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:  tf.Tensor(b'\"what is the difference between these two ways to create an element? var a = document.createelement(\\'div\\');..a.id = \"\"mydiv\"\";...and..var a = document.createelement(\\'div\\').id = \"\"mydiv\"\";...what is the difference between them such that the first one works and the second one doesn\\'t?\"\\n', shape=(), dtype=string)\n",
            "Label:  tf.Tensor(2, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAKWKzM-XuC-",
        "outputId": "a48abea5-e387-4ce9-9da6-833246e544ee"
      },
      "source": [
        "print(\"'binary' vectorized question:\", binary_vectorize_text(first_question, first_label)[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'binary' vectorized question: tf.Tensor([[1. 1. 0. ... 0. 0. 0.]], shape=(1, 10000), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFDhB7I3ZaDX",
        "outputId": "855cefff-1c03-4e4d-9da3-013cc85e679d"
      },
      "source": [
        "print(\"'int' vectorized question:\", int_vectorize_text(first_question, first_label)[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'int' vectorized question: tf.Tensor(\n",
            "[[ 55   6   2 410 211 229 121 895   4 124  32 245  43   5   1   1   5   1\n",
            "    1   6   2 410 211 191 318  14   2  98  71 188   8   2 199  71 178   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]], shape=(1, 250), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0-kkw09Zgfg"
      },
      "source": [
        "위에서 보았듯이 `binary` 모드는 maximum vocabulary size내에 속해 있는지 여부를 0과 1로 나타내는 반면, `int` 모드는 각 토큰을 해당하는 정수로 변환해서 질서를 보존합니다.\n",
        "\n",
        "`.get_vocabulary()` 메서드를 통해 정수에 대응되는 문자가 무엇인지 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlfEEq9iZfzL",
        "outputId": "ac7e9565-0459-44fa-dbbb-5e6cd0e6d29e"
      },
      "source": [
        "print('1289 ---> ', int_vectorize_layer.get_vocabulary()[1289])\n",
        "print('313 ---> ', int_vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(int_vectorize_layer.get_vocabulary())))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1289 --->  roman\n",
            "313 --->  source\n",
            "Vocabulary size: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFXPyjFccOLp"
      },
      "source": [
        "이제 모델을 훈련시킬 준비가 거의 끝났습니다. 마지막 전처리 과정으로 `TextVectorization` 레이어를 훈련, 검증, 테스트 데이터세트에 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUlwSx8CbklL"
      },
      "source": [
        "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
        "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
        "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
        "\n",
        "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
        "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
        "int_test_ds = raw_test_ds.map(int_vectorize_text)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pls_RMcddaq"
      },
      "source": [
        "### 훈련 성능 향상시키기\n",
        "\n",
        "I/O가 블로킹되지 않도록 데이터를 로딩할 때 필수적으로 해야할 2가지가 있습니다.\n",
        "\n",
        "+ `.cache()` : 디스크로부터 데이터를 로드한 후에 메모리에 남겨놓도록 합니다. 모델을 훈련할 때 데이터세트가 소위 병목(병의 목 부분처럼 좁아서 흐름이 지연되는 현상) 현상이 되는 것을 방지할 수 있습니다. 데이터세트의 크기가 너무 커서 메모리 안에 훈련될 수 없다면, 이 방법이 작은 파일들을 읽는 것보다 훨씬 효율적으로 데이터를 읽어들일 수 있는 캐시를 생성해서 해결할 수 있습니다.\n",
        "\n",
        "+ `.prefetch()` : 훈련하는 동안 데이터의 전처리 과정과 모델 학습 작용이 동시에 이루어지도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt2TxpbbfffT"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE  # 컴퓨터의 CPU에 맞는 적절한 구동 코어수 결정\n",
        "# 데이터 세트를 캐시 및 프리패치하는 함수 생성\n",
        "def configure_dataset(dataset):\n",
        "    return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWhwKoH5fqSE"
      },
      "source": [
        "binary_train_ds = configure_dataset(binary_train_ds)\n",
        "binary_val_ds = configure_dataset(binary_val_ds)\n",
        "binary_test_ds = configure_dataset(binary_test_ds)\n",
        "\n",
        "int_train_ds = configure_dataset(int_train_ds)\n",
        "int_val_ds = configure_dataset(int_val_ds)\n",
        "int_test_ds = configure_dataset(int_test_ds)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rnCNv7GgEUJ"
      },
      "source": [
        "### 모델 훈련하기\n",
        "\n",
        "이제 신경망을 구성할 차례입니다.\n",
        "\n",
        "`binary` 벡터화 데이터를 이용해 간단한 bag-of-words 선형 모델을 학습해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RLAuspBf83h",
        "outputId": "3fa43876-4b81-4248-8aa3-89434f70047f"
      },
      "source": [
        "binary_model = tf.keras.Sequential([layers.Dense(4)])\n",
        "binary_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history = binary_model.fit(\n",
        "    binary_train_ds, \n",
        "    validation_data=binary_val_ds, \n",
        "    epochs=10\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "200/200 [==============================] - 3s 15ms/step - loss: 1.1237 - accuracy: 0.6433 - val_loss: 0.9194 - val_accuracy: 0.7788\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.7807 - accuracy: 0.8206 - val_loss: 0.7538 - val_accuracy: 0.8006\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.6288 - accuracy: 0.8592 - val_loss: 0.6675 - val_accuracy: 0.8150\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.5351 - accuracy: 0.8845 - val_loss: 0.6137 - val_accuracy: 0.8219\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.4690 - accuracy: 0.9022 - val_loss: 0.5767 - val_accuracy: 0.8313\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.4186 - accuracy: 0.9175 - val_loss: 0.5499 - val_accuracy: 0.8319\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.3783 - accuracy: 0.9270 - val_loss: 0.5297 - val_accuracy: 0.8350\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.3449 - accuracy: 0.9353 - val_loss: 0.5140 - val_accuracy: 0.8356\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.3167 - accuracy: 0.9425 - val_loss: 0.5017 - val_accuracy: 0.8381\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 1s 3ms/step - loss: 0.2923 - accuracy: 0.9484 - val_loss: 0.4919 - val_accuracy: 0.8400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uylTxtjKg7ud"
      },
      "source": [
        "이제 `int` 벡터화 레이어를 사용해 `1D ConvNet`을 빌드해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW86kv4UgsqG"
      },
      "source": [
        "def create_model(vocab_size, num_labels):\n",
        "    model = tf.keras.Sequential([\n",
        "                                 layers.Embedding(vocab_size, 64, mask_zero=True),\n",
        "                                 layers.Conv1D(64, 5, padding='valid', activation='relu', strides=2),\n",
        "                                 layers.GlobalMaxPooling1D(),\n",
        "                                 layers.Dense(num_labels)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-j0lNH4iV81",
        "outputId": "eef2cda8-955a-4bf7-d2e9-ce6ae61a915a"
      },
      "source": [
        "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4)\n",
        "int_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "200/200 [==============================] - 9s 41ms/step - loss: 1.1398 - accuracy: 0.5200 - val_loss: 0.7533 - val_accuracy: 0.6862\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.6188 - accuracy: 0.7597 - val_loss: 0.5419 - val_accuracy: 0.7987\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 6s 31ms/step - loss: 0.3758 - accuracy: 0.8806 - val_loss: 0.4778 - val_accuracy: 0.8219\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.2109 - accuracy: 0.9511 - val_loss: 0.4729 - val_accuracy: 0.8213\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 6s 30ms/step - loss: 0.1067 - accuracy: 0.9831 - val_loss: 0.4939 - val_accuracy: 0.8194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBjANO7elY_v"
      },
      "source": [
        "두 모델을 비교해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yceMNnWoirpu",
        "outputId": "5aa0e28d-c135-412f-e648-37d71767a9c5"
      },
      "source": [
        "print('Linear model on binary vectorized data:')\n",
        "print(binary_model.summary())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear model on binary vectorized data:\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 4)                 40004     \n",
            "=================================================================\n",
            "Total params: 40,004\n",
            "Trainable params: 40,004\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efbuUch6ljRC",
        "outputId": "a3cd82e6-3fd3-421e-8304-43f954e3c6b8"
      },
      "source": [
        "print(\"ConvNet model on int vectorized data:\")\n",
        "print(int_model.summary())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet model on int vectorized data:\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          640064    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 64)          20544     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Globa  (None, 64)                0         \n",
            "lMaxPooling1D)                                                   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 660,868\n",
            "Trainable params: 660,868\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAtWT_nltnQ"
      },
      "source": [
        "두 모델을 테스트 데이터를 이용해 평가해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YupX7-wvlsU2",
        "outputId": "d9c66f5b-8752-48ce-b269-14ca4200bd38"
      },
      "source": [
        "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n",
        "int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n",
        "\n",
        "print(\"Binary model accuracy: {:2.2%}\".format(binary_accuracy))\n",
        "print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 3s 11ms/step - loss: 0.5182 - accuracy: 0.8142\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 0.5196 - accuracy: 0.8075\n",
            "Binary model accuracy: 81.42%\n",
            "Int model accuracy: 80.75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FutdOCOCm9hy"
      },
      "source": [
        "주의: 이 데이터세트는 다소 간단한 분류 문제를 보여줍니다. 더 복잡한 데이터세트와 문제들은 전처리 전략과 모델 아키텍처에 따라서 사소하지만 중요한 변화를 가져옵니다. 여러 다른 하이퍼파라미터와 에포크를 시도해보면서 다양한 접근을 해보는 것이 중요합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egG_nGp_nmXi"
      },
      "source": [
        "### 모델 익스포트하기\n",
        "\n",
        "앞서 텍스트를 모델에 입력하기 전에 `TextVectorization` 레이어를 적용했습니다. 만약 전처리 되지 않은 문자열을 바로 모델에 입력하고 싶다면, `TextVectorization` 레이어를 모델 안에 넣으면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRBfowY7mwQg",
        "outputId": "a480cde5-f72b-4956-cee7-f8edd0ececab"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "                                    binary_vectorize_layer,\n",
        "                                    binary_model,\n",
        "                                    layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4908: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 3s 10ms/step - loss: 0.5182 - accuracy: 0.8142\n",
            "Accuracy: 81.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBHL6EsOpE4C"
      },
      "source": [
        "이미 `binary_vectorize_layer`와 `binary_model`은 앞서 훈련이 되었으므로 새로 만들어진 `export_model`을 따로 훈련하진 않았습니다.\n",
        "\n",
        "이렇게 함으로써 모델에 전처리 되지 않은 문자열을 입력으로 넣고 `model.predict`를 사용해 각 레이블에 대한 확률을 예측할 수 있습니다.\n",
        "\n",
        "최댓값을 가지는 레이블을 찾는 함수를 정의해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e54iLqcoym7"
      },
      "source": [
        "def get_string_labels(predicted_scores_batch):\n",
        "    predicted_int_labels = tf.argmax(predicted_scores_batch, axis=1)\n",
        "    predicted_labels = tf.gather(raw_train_ds.class_names, predicted_int_labels)\n",
        "    return predicted_labels"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMNJUJnSqQP7"
      },
      "source": [
        "### 새로운 데이터를 사용해 결과 예측하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ykzpbupqO7f",
        "outputId": "63019e65-9185-4b30-91c5-aa93f4174819"
      },
      "source": [
        "inputs = [\n",
        "          \"how do I extract keys from a dict into a list?\",  # python\n",
        "          \"debug public static void main(string[] args) {...}\",  # java\n",
        "]\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = get_string_labels(predicted_scores)\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "    print(\"Question: \", input)\n",
        "    print(\"Predicted label: \", label.numpy())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:  how do I extract keys from a dict into a list?\n",
            "Predicted label:  b'python'\n",
            "Question:  debug public static void main(string[] args) {...}\n",
            "Predicted label:  b'java'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unTvPBdcrRik"
      },
      "source": [
        "텍스트 전처리 로직을 모델 안에 포함하는 것은 새로운 데이터를 예측할 때 훨씬 단순화하고 잠재적인 [train/test skew](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew)를 감소시켜줍니다.\n",
        "\n",
        "`TextVectorization` 레이어를 어디에 적용시키느냐에 따라 성능에 차이가 생긴다는 점을 명심해야합니다. 이 레이어를 모델 밖에서 사용하면 CPU 처리와 GPU에서의 훈련 데이터 버퍼링이 동시에 일어나지 않습니다. 그래서 GPU에서 모델을 훈련한다면, 모델의 좋은 훈련 성능을 위한 선택이 될 수 있습니다. 그리고 나서 `TextVectorization` 레이어를 모델 안으로 위치를 바꿔주면 앞으로 새로운 데이터를 예측할 때 좋은 성능을 낼 수 있을 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ebFM6JAP5xE"
      },
      "source": [
        "## 2. 일리아드 번역 저자 예측하기\n",
        "\n",
        "이번에는 `tf.data.TextLineDataset`을 이용해 텍스트 파일로부터 example을 로드하고 `tf.text`를 이용해 데이터를 전처리 할 것입니다. example은 Homer의 일리아드라는 작품을 영문으로 번역한 3개의 번역본입니다. 텍스트 한 줄이 주어졌을 때, 번역가가 누구인지를 판별하는 모델을 훈련시킬 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I68XJ2hvRMsD"
      },
      "source": [
        "### 데이터세트 다운로드 및 살펴보기\n",
        "\n",
        "3명의 번역가는 아래와 같습니다.\n",
        "- [William Cowper](https://en.wikipedia.org/wiki/William_Cowper) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt)\n",
        "\n",
        "- [Edward, Earl of Derby](https://en.wikipedia.org/wiki/Edward_Smith-Stanley,_14th_Earl_of_Derby) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt)\n",
        "\n",
        "- [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_%28novelist%29) — [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt)\n",
        "\n",
        "텍스트 파일은 이미 헤더, 풋터, 라인 넘버, 챕터 타이틀을 제거하는 전처리 작업을 거쳤습니다. 변환된 파일들을 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX8c3aAwSI4J",
        "outputId": "c6efcdc6-4acf-4a3e-d021-604711c55db3"
      },
      "source": [
        "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "for name in FILE_NAMES:\n",
        "    text_dir = utils.get_file(name, origin=DIRECTORY_URL + name)\n",
        "\n",
        "parent_dir = pathlib.Path(text_dir).parent\n",
        "list(parent_dir.iterdir())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.keras/datasets/butler.txt'),\n",
              " PosixPath('/root/.keras/datasets/derby.txt'),\n",
              " PosixPath('/root/.keras/datasets/cowper.txt')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMBJLIfsTuXQ"
      },
      "source": [
        "### 데이터세트 로드하기\n",
        "\n",
        "`TextLineDataset`는 오리지널 파일에서 텍스트를 한 줄 씩 example로 만들어서 `tf.data.Dataset`을 생성하도록 고안되었습니다. 반면에 `text_dataset_from_directory`는 파일 전체의 내용을 하나의 example로 만듭니다. `TextLineDataset`은 줄 단위로 되어있는 텍스트 데이터를 다루는데 유용합니다.\n",
        "\n",
        "for문으로 반복처리하면서 3개의 텍스트 파일에서 데이터세트를 로드합니다. 각 example은 각각의 레이블에 대응되어야하므로 `tf.data.Dataset.map`을 이용해서 (example, label) 쌍을 줄 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTHQeNBiVd43"
      },
      "source": [
        "def labeler(example, index):\n",
        "    return example, tf.cast(index, tf.int64)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boOK5FIMVs2o"
      },
      "source": [
        "labeled_data_sets = []\n",
        "\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "    # 3개의 파일을 TextLineDataset 처리해줍니다.\n",
        "    lines_dataset = tf.data.TextLineDataset(str(parent_dir/file_name))\n",
        "    # (example, label) 쌍을 만듭니다.\n",
        "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
        "    # labeled_data_sets에 추가합니다.\n",
        "    labeled_data_sets.append(labeled_dataset)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehRPo46LWk-h"
      },
      "source": [
        "이제 레이블된 데이터세트들을 결합해서 하나의 데이터세트로 만들고 섞어줄 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Tm-QIiWjxm"
      },
      "source": [
        "BUFFER_SIZE = 50000\n",
        "BATCH_SIZE = 64\n",
        "VALIDATION_SIZE = 5000"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hok4RdBbWxm1"
      },
      "source": [
        "all_labeled_data = labeled_data_sets[0]  # 첫 번째 파일의 데이터세트를 먼저 넣어줍니다.\n",
        "for labeled_dataset in labeled_data_sets[1:]:  # 두 번째 파일부터 반복하며 concatenate를 이용해 데이터세트를 추가합니다.\n",
        "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
        "# 결합된 데이터세트 내의 데이터들을 한 번 섞어줍니다.\n",
        "all_labeled_data = all_labeled_data.shuffle(\n",
        "    BUFFER_SIZE, reshuffle_each_iteration=False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlxgDpWLYoDb"
      },
      "source": [
        "하나가 된 데이터세트 내의 10개의 데이터를 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waIAB5qaYcAx",
        "outputId": "674ea333-e7a6-4769-cb46-001939867c96"
      },
      "source": [
        "for text, label in all_labeled_data.take(10):\n",
        "    print(\"Sentence: \", text.numpy())\n",
        "    print(\"Label: \", label.numpy())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  b'The dust, put on their tunics. Then again'\n",
            "Label:  0\n",
            "Sentence:  b\"But miss'd Patroclus; the innocuous point,\"\n",
            "Label:  0\n",
            "Sentence:  b\"He said; and from th' applauding ranks of Greece\"\n",
            "Label:  1\n",
            "Sentence:  b'The Gods, or our supineness, succor Troy.'\n",
            "Label:  0\n",
            "Sentence:  b\"And struck the circle of AEneas' shield\"\n",
            "Label:  1\n",
            "Sentence:  b'Of Agamemnon, where the Chiefs, perchance,'\n",
            "Label:  0\n",
            "Sentence:  b'For him, with confident persuasion all'\n",
            "Label:  0\n",
            "Sentence:  b\"His mother, Juno, white-arm'd Queen of Heav'n:\"\n",
            "Label:  1\n",
            "Sentence:  b'the Trojans. Therefore he drew back, and the Trojans flung fire upon'\n",
            "Label:  2\n",
            "Sentence:  b'Noblest of all our host! bear with my grief,'\n",
            "Label:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g-F_CiOYV5k"
      },
      "source": [
        "### 훈련을 위한 데이터세트 준비\n",
        "\n",
        "Keras의 `TextVectorization` 레이어를 이용해 텍스트 데이터세트를 전처리하는 대신, 이번에는 `tf.text` API를 이용해 텍스트 데이터를 표준화 및 토큰화하고, 단어 사전을 빌드한 후 `StaticVocabularyTable`를 이용해 토큰을 정수에 매핑시켜보도록 하겠습니다.\n",
        "\n",
        "먼저 `tf.text.UnicodeScriptTokenizer`로 토큰화기를 만들겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUlZ70bVaB05"
      },
      "source": [
        "tokenizer = tf_text.UnicodeScriptTokenizer()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrNUBQkSaH6p"
      },
      "source": [
        "이제 데이터를 소문자화 및 토큰화하는 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZtEOTQDaGEB"
      },
      "source": [
        "def tokenize(text, unused_label):\n",
        "    lower_case = tf_text.case_fold_utf8(text)\n",
        "    return tokenizer.tokenize(lower_case)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLOPsgXfajyF"
      },
      "source": [
        "`tf.data.Dataset.map`을 이용해 데이터세트를 토큰화하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMEnui7JaxhS",
        "outputId": "518b8d34-5291-42c0-e5a2-522e6f1236a5"
      },
      "source": [
        "tokenized_ds = all_labeled_data.map(tokenize)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
            "Instructions for updating:\n",
            "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJNJvJJsb6gh"
      },
      "source": [
        "토큰화된 데이터세트 중 5개를 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXr0pI5_cGGR",
        "outputId": "5dfc41c7-70e2-4577-e96f-207821de51fa"
      },
      "source": [
        "for text_batch in tokenized_ds.take(5):\n",
        "    print(\"Tokens: \", text_batch.numpy())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens:  [b'the' b'dust' b',' b'put' b'on' b'their' b'tunics' b'.' b'then' b'again']\n",
            "Tokens:  [b'but' b'miss' b\"'\" b'd' b'patroclus' b';' b'the' b'innocuous' b'point'\n",
            " b',']\n",
            "Tokens:  [b'he' b'said' b';' b'and' b'from' b'th' b\"'\" b'applauding' b'ranks' b'of'\n",
            " b'greece']\n",
            "Tokens:  [b'the' b'gods' b',' b'or' b'our' b'supineness' b',' b'succor' b'troy'\n",
            " b'.']\n",
            "Tokens:  [b'and' b'struck' b'the' b'circle' b'of' b'aeneas' b\"'\" b'shield']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCHCgi4xgWow"
      },
      "source": [
        "다음으로, 단어의 언급 빈도수에 따라 토큰을 정렬하고, `VOCAB_SIZE` 제한을 유지하면서 단어 사전을 빌드할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGBM8G7WgxW0",
        "outputId": "d0a1e90c-750b-4fef-aa5f-6a81d7ccec64"
      },
      "source": [
        "tokenized_ds = configure_dataset(tokenized_ds)\n",
        "\n",
        "vocab_dict = collections.defaultdict(lambda: 0)\n",
        "for toks in tokenized_ds.as_numpy_iterator():\n",
        "    for tok in toks:\n",
        "        vocab_dict[tok] += 1\n",
        "# 단어의 언급 빈도수에 따라 정렬\n",
        "vocab = sorted(vocab_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "vocab = [token for token, count in vocab]\n",
        "vocab = vocab[:VOCAB_SIZE]  # 가장 많이 언급되는 단어부터 최대 10000개의 단어까지\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size: \", vocab_size)\n",
        "print(\"First five vocab entries: \", vocab[:5])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  10000\n",
            "First five vocab entries:  [b',', b'the', b'and', b\"'\", b'of']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B3mdIOji-Cm"
      },
      "source": [
        "토큰을 정수로 변환하기 위해서 `StaticVocabularyTable`을 생성하는 vocab 세트를 이용합니다. 토큰을 정수 범위 [2, vocab_size + 2]로 변환할 것입니다. 0과 1을 빼는 이유는 0은 패딩을 위한 값, 1은 out-of-vocabulary(OOV)를 위한 값으로 사용될 것이기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5i6iKeLi7Db"
      },
      "source": [
        "keys = vocab  # 단어\n",
        "values = range(2, len(vocab) + 2)  # 정수 : 2 ~ 10,002\n",
        "# 단어와 정수를 대응시키기\n",
        "init = tf.lookup.KeyValueTensorInitializer(\n",
        "    keys, values, key_dtype=tf.string, value_dtype=tf.int64\n",
        ")\n",
        "\n",
        "num_oov_buckets = 1  # 사전을 벗어나는 단어는 1로 처리\n",
        "vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets)  # 룩-업 테이블(사전) 구성"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGoXxfkSlUJB"
      },
      "source": [
        "마지막으로, 표준화, 토큰화, 벡터화시키는 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfQ7h6ubm1sZ"
      },
      "source": [
        "def preprocess_text(text, label):\n",
        "    standardized = tf_text.case_fold_utf8(text)  # 소문자화\n",
        "    tokenized = tokenizer.tokenize(standardized)  # 토큰화\n",
        "    vectorized = vocab_table.lookup(tokenized)  # 벡터화\n",
        "    return vectorized, label"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRvboFQYn01S"
      },
      "source": [
        "하나의 (example, label) 쌍을 뽑아내서 벡터화까지 된 결과를 살펴봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0zqQHGmnSLf",
        "outputId": "b1edaef5-3926-4c24-8997-472b2acc17a6"
      },
      "source": [
        "example_text, example_label = next(iter(all_labeled_data))\n",
        "print(\"Sentence: \", example_text.numpy())\n",
        "vectorized_text, example_label = preprocess_text(example_text, example_label)\n",
        "print(\"Vectorized sentence: \", vectorized_text.numpy())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence:  b'The dust, put on their tunics. Then again'\n",
            "Vectorized sentence:  [   3  317    2  383   22   30 4110    7   33  161]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW7DUzo-ojih"
      },
      "source": [
        "`tf.data.Dataset.map`을 통해 벡터화된 데이터세트로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoEewu-oooUA"
      },
      "source": [
        "all_encoded_data = all_labeled_data.map(preprocess_text)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OER1RspotKs"
      },
      "source": [
        "### 데이터세트에서 훈련과 테스트 세트 분리하기\n",
        "\n",
        "Keras의 `TextVectorization` 레이어를 사용할 때는 패딩 처리된 벡터화된 데이터가 필요했습니다. 하지만 현재 데이터세트는 데이터가 모두 같은 크기와 모양을 가질 필요가 없습니다.\n",
        "\n",
        "데이터세트를 훈련과 검증 세트로 나누겠습니다. 이후 훈련과 검증 데이터세트를 배치화 시키겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5POP0N3DosoV"
      },
      "source": [
        "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE)\n",
        "validation_data = all_encoded_data.take(VALIDATION_SIZE)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "980IhRWPtKjI"
      },
      "source": [
        "train_data = train_data.padded_batch(BATCH_SIZE)\n",
        "validation_data = validation_data.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFNGH50qtVwi"
      },
      "source": [
        "이제 `train_data`와 `validation_data`는 (example, label)의 쌍의 집합이 아니라 배치의 집합입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yicjSOrtVPo",
        "outputId": "8bab984a-95b2-45f4-cfa0-30f79ba64f3d"
      },
      "source": [
        "sample_text, sample_labels = next(iter(validation_data))\n",
        "print(\"Text batch shape: \", sample_text.shape)\n",
        "print(\"Label batch shape: \", sample_labels.shape)\n",
        "print(\"First text example: \", sample_text[0])\n",
        "print(\"First label example: \", sample_labels[0])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text batch shape:  (64, 17)\n",
            "Label batch shape:  (64,)\n",
            "First text example:  tf.Tensor(\n",
            "[   3  317    2  383   22   30 4110    7   33  161    0    0    0    0\n",
            "    0    0    0], shape=(17,), dtype=int64)\n",
            "First label example:  tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3r3I1rUuE5G"
      },
      "source": [
        "`0`과 `1`은 패딩과 OOV를 위한 토큰 값이므로 `vocab_size`를 2 늘려준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYFaKJgIuDzm"
      },
      "source": [
        "vocab_size += 2"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHKUETPpuQyT"
      },
      "source": [
        "데이터셋을 `configure`해서 더 좋은 성능을 가질 수 있도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-RJgqjeuQRz"
      },
      "source": [
        "train_data = configure_dataset(train_data)\n",
        "validation_data = configure_dataset(validation_data)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoR7HR-RueIs"
      },
      "source": [
        "### 모델 훈련시키기\n",
        "\n",
        "위에서 만들어놓았던 모델 생성 함수를 이용해 모델을 만들어 훈련시킬 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjXgw47TucaD",
        "outputId": "0de67be7-5d49-4046-eaf9-5fb3d1aedb77"
      },
      "source": [
        "model = create_model(vocab_size=vocab_size, num_labels=3)\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history = model.fit(train_data, validation_data=validation_data, epochs=3)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "697/697 [==============================] - 43s 23ms/step - loss: 0.5245 - accuracy: 0.7625 - val_loss: 0.3878 - val_accuracy: 0.8354\n",
            "Epoch 2/3\n",
            "697/697 [==============================] - 10s 14ms/step - loss: 0.2855 - accuracy: 0.8849 - val_loss: 0.3776 - val_accuracy: 0.8438\n",
            "Epoch 3/3\n",
            "697/697 [==============================] - 10s 14ms/step - loss: 0.1913 - accuracy: 0.9278 - val_loss: 0.4142 - val_accuracy: 0.8402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnfYXp2mu70x",
        "outputId": "d7bf3ae1-5a2f-4d86-c6fd-8245ebf48230"
      },
      "source": [
        "loss, accuracy = model.evaluate(validation_data)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 2ms/step - loss: 0.4142 - accuracy: 0.8402\n",
            "Loss:  0.41424307227134705\n",
            "Accuracy: 84.02%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxPmQKVcvYBK"
      },
      "source": [
        "### 모델 익스포트하기\n",
        "\n",
        "정제되지 않은 문자열을 모델의 input으로 받고 싶을 경우, `TextVectorization` 레이어를 만들어서 우리가 직접 만들어보았던 전처리 함수의 기능을 수행하도록 할 수 있습니다. 이미 룩업 테이블을 만들면서 단어를 훈련시켜 놓았기 때문에 `adapt`를 수행하는 대신에 `set_vocabulary`를 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b__faiJdv7B8"
      },
      "source": [
        "preprocess_layer = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    standardize=tf_text.case_fold_utf8,\n",
        "    split=tokenizer.tokenize,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "preprocess_layer.set_vocabulary(vocab)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr_9_Dk3wgLE"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "                                    preprocess_layer,\n",
        "                                    model,\n",
        "                                    layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dANf7ERMwwi_",
        "outputId": "4124b5b8-9588-4514-b4bf-f7a6061367ea"
      },
      "source": [
        "test_ds = all_labeled_data.take(VALIDATION_SIZE).batch(BATCH_SIZE)\n",
        "test_ds = configure_dataset(test_ds)\n",
        "loss, accuracy = export_model.evaluate(test_ds)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4908: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 10s 27ms/step - loss: 0.5812 - accuracy: 0.7808\n",
            "Loss:  0.5812350511550903\n",
            "Accuracy: 78.08%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5-qswCFxL_8"
      },
      "source": [
        "### 새로운 데이터로 실행시켜보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSyStoEGxAsa",
        "outputId": "a372da65-c1e0-4785-aece-0b8cf2f96bc0"
      },
      "source": [
        "inputs = [\n",
        "    \"Join'd to th' Ionians with their flowing robes,\",  # Label: 1\n",
        "    \"the allies, and his armour flashed about him so that he seemed to all\",  # Label: 2\n",
        "    \"And with loud clangor of his arms he fell.\",  # Label: 0\n",
        "]\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = tf.argmax(predicted_scores, axis=1)\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "    print(\"Question: \", input)\n",
        "    print(\"Predicted label: \", label.numpy())"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:  Join'd to th' Ionians with their flowing robes,\n",
            "Predicted label:  1\n",
            "Question:  the allies, and his armour flashed about him so that he seemed to all\n",
            "Predicted label:  2\n",
            "Question:  And with loud clangor of his arms he fell.\n",
            "Predicted label:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5JidN-xxyKg"
      },
      "source": [
        "### TensorFlow Datasets(TFDS)를 사용해 더 많은 데이터세트 다운로드하기\n",
        "\n",
        "[TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview)에서 더 많은 데이터세트를 다운로드 할 수 있습니다. [IMDB Large Movie Review dataset](https://www.tensorflow.org/datasets/catalog/imdb_reviews)를 다운로드 받고 이진 분류를 위한 모델 훈련에 사용해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhIJ2n7wxjpr"
      },
      "source": [
        "train_ds = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split='train[:80%]',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True\n",
        ")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewNINV8byg_Q"
      },
      "source": [
        "val_ds = tfds.load(\n",
        "    'imdb_reviews',\n",
        "    split='train[80%:]',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True\n",
        ")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEmMm_PZypaj"
      },
      "source": [
        "for review_batch, label_batch in val_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(\"Review: \", review_batch[i].numpy())\n",
        "        print(\"Label: \", label_batch[i].numpy())"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTai6PYV0Zkq"
      },
      "source": [
        "### 훈련을 위한 데이터세트 준비하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqZjnajG0cQv"
      },
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH\n",
        ")\n",
        "\n",
        "train_text = train_ds.map(lambda text, labels: text)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79HHVeOX0v35"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQmktI7403vR"
      },
      "source": [
        "train_ds = train_ds.map(vectorize_text)\n",
        "val_ds = val_ds.map(vectorize_text)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwOGgO6Z0-Dj"
      },
      "source": [
        "train_ds = configure_dataset(train_ds)\n",
        "val_ds = configure_dataset(val_ds)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQftfbLd1OSl"
      },
      "source": [
        "### 모델 훈련시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG9jwQao1PY7",
        "outputId": "0842ad62-fd48-4177-9ea3-edda7a57b523"
      },
      "source": [
        "model = create_model(vocab_size=VOCAB_SIZE+1, num_labels=1)\n",
        "model.summary()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 64)          640064    \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 64)          20544     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glo  (None, 64)                0         \n",
            "balMaxPooling1D)                                                 \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 660,673\n",
            "Trainable params: 660,673\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGaQ04ab1U4f"
      },
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocgpEd_x1ee9",
        "outputId": "b8be2771-b7fc-4659-f361-b79c828f32db"
      },
      "source": [
        "history = model.fit(train_ds, validation_data=val_ds, epochs=3)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "313/313 [==============================] - 20s 60ms/step - loss: 0.5480 - accuracy: 0.6554 - val_loss: 0.3790 - val_accuracy: 0.8250\n",
            "Epoch 2/3\n",
            "313/313 [==============================] - 15s 49ms/step - loss: 0.3025 - accuracy: 0.8660 - val_loss: 0.3209 - val_accuracy: 0.8586\n",
            "Epoch 3/3\n",
            "313/313 [==============================] - 16s 50ms/step - loss: 0.1856 - accuracy: 0.9269 - val_loss: 0.3264 - val_accuracy: 0.8634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ha8SxTI1is8",
        "outputId": "1f45772e-00ea-47b4-c245-0965b993d665"
      },
      "source": [
        "loss, accuracy = model.evaluate(val_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 13ms/step - loss: 0.3264 - accuracy: 0.8634\n",
            "Loss:  0.32642149925231934\n",
            "Accuracy: 86.34%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USoFaClr1yHo"
      },
      "source": [
        "### 모델 익스포트하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFevwRRt110K"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "                                    vectorize_layer,\n",
        "                                    model,\n",
        "                                    layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZfWlrQP2Dkw",
        "outputId": "dfb39b71-60b9-490f-907c-440d603ac526"
      },
      "source": [
        "inputs = [\n",
        "    \"This is a fantastic movie.\",\n",
        "    \"This is a bad movie.\",\n",
        "    \"This movie was so bad that it was good.\",\n",
        "    \"I will never say yes to watching this movie.\"\n",
        "]\n",
        "\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = [int(round(x[0])) for x in predicted_scores]\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "    print(\"Question: \", input)\n",
        "    print(\"Predicted label: \", label)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:  This is a fantastic movie.\n",
            "Predicted label:  1\n",
            "Question:  This is a bad movie.\n",
            "Predicted label:  0\n",
            "Question:  This movie was so bad that it was good.\n",
            "Predicted label:  0\n",
            "Question:  I will never say yes to watching this movie.\n",
            "Predicted label:  0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}