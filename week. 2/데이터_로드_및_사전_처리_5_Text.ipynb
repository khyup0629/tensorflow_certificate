{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "데이터 로드 및 사전 처리 #5 - Text",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kfiyXXm9jMr"
      },
      "source": [
        "# 텍스트 로드하기\n",
        "\n",
        "이번 튜토리얼에서는 텍스트를 로드하고 전처리하는 두 가지 방법을 소개합니다.\n",
        "+ keras 유틸리티와 레이어 사용하기\n",
        "+ 텍스트 파일을 로드하기 위해 `tf.data.TextLineDataset`를 사용하고, 데이터를 전처리하기 위해 `tf.text`를 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlFNuUlL9HrV",
        "outputId": "58ba0fbc-e522-4b39-addd-a68aadb16e9b"
      },
      "source": [
        "!pip uninstall -y tensorflow tf-nightly keras\n",
        "\n",
        "!pip install -q -U tf-nightly\n",
        "!pip install -q -U tensorflow-text-nightly"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.5.0:\n",
            "  Successfully uninstalled tensorflow-2.5.0\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Uninstalling Keras-2.4.3:\n",
            "  Successfully uninstalled Keras-2.4.3\n",
            "\u001b[K     |████████████████████████████████| 453.3MB 35kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.5MB 29.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 38.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2MB 30.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 12.8MB 230kB/s \n",
            "\u001b[K     |████████████████████████████████| 471kB 52.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.3MB 5.2MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FqyPQCl-WEh"
      },
      "source": [
        "import collections, pathlib, re, string\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvT7k5Zc-9Zf"
      },
      "source": [
        "## 1. 스택 오버플로우를 위한 태그 예측하기\n",
        "\n",
        "스택 오버플로우로부터 프로그래밍 질문 데이터세트를 다운로드합니다. 각 질문(어떻게 딕셔너리를 값에 따라 정렬하는가?)은 하나의 태그(`Python`, `CSharp`, `JavaScript`, `Java` 중 하나)에 레이블되어 있습니다.\n",
        "\n",
        "이번 과제는 질문에 대한 태그를 예측하는 모델을 개발하는 멀티 클래스 분류 작업입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klwQGFawAFKf"
      },
      "source": [
        "### 데이터세트 다운로드 및 살펴보기\n",
        "\n",
        "이제 데이터세트를 다운로드하고 딕셔너리 구조를 살펴봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtHsXlnvANCy",
        "outputId": "9d2b5d87-55a8-487e-c500-66b15befcc8f"
      },
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "dataset_dir = tf.keras.utils.get_file(\n",
        "    origin=data_url,\n",
        "    untar=True,\n",
        "    cache_dir='stack_overflow',\n",
        "    cache_subdir=''\n",
        ")\n",
        "\n",
        "# 경로를 PosixPath로 나타내도록 한다. .parent는 가장 마지막 파일 바로 위까지의 경로를 나타낸다.\n",
        "dataset_dir = pathlib.Path(dataset_dir).parent"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\n",
            "6053888/6053168 [==============================] - 0s 0us/step\n",
            "6062080/6053168 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll3bLJ5gBuB1",
        "outputId": "9616ecde-a9f3-4004-8c77-fb51f04bd408"
      },
      "source": [
        "list(dataset_dir.iterdir())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/tmp/.keras/README.md'),\n",
              " PosixPath('/tmp/.keras/test'),\n",
              " PosixPath('/tmp/.keras/train'),\n",
              " PosixPath('/tmp/.keras/stack_overflow_16k.tar.gz')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhEwUcr6ByOR",
        "outputId": "c3a1b177-79a8-4bca-d834-d41330687d8d"
      },
      "source": [
        "train_dir = dataset_dir/'train'\n",
        "list(train_dir.iterdir())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/tmp/.keras/train/javascript'),\n",
              " PosixPath('/tmp/.keras/train/python'),\n",
              " PosixPath('/tmp/.keras/train/csharp'),\n",
              " PosixPath('/tmp/.keras/train/java')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NpbX7UFEJlq"
      },
      "source": [
        "`train/csharp`, `train/java`, `train/python`, `train/javascript` 디렉토리에는 많은 텍스트 파일들이 저장되어 있습니다. 각 파일들은 스택 오버플로우 질문들입니다. 파일 하나의 데이터를 살펴봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFovTod4B-KD",
        "outputId": "518bbedd-8053-4536-f501-858c52b48a9a"
      },
      "source": [
        "sample_file = train_dir/'python/1755.txt'\n",
        "print(open(sample_file).read())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "why does this blank program print true x=true.def stupid():.    x=false.stupid().print x\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w6Sfke8E33v"
      },
      "source": [
        "### 데이터세트 로드 및 구성하기\n",
        "\n",
        "이제 디스크에서 데이터를 로드하고 훈련에 적합한 포맷으로 만들 것입니다. `tf.data.Dataset`을 생성하기 위해 `text_dataset_from_directory`를 사용하겠습니다. `tf.data`는 입력 파이프라인을 빌드하는데 아주 강력한 툴입니다.\n",
        "\n",
        "`preprocessing.text_dataset_from_directory`는 아래와 같은 디렉토리 구조를 가집니다.\n",
        "\n",
        "```\n",
        "train/\n",
        "...csharp/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...java/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...javascript/\n",
        "......1.txt\n",
        "......2.txt\n",
        "...python/\n",
        "......1.txt\n",
        "......2.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPxvVKGuG-Qe"
      },
      "source": [
        "머신 러닝 실험을 진행할 때, 데이터세트를 훈련, 검증, 테스트의 3가지로 나누는 것이 가장 좋습니다. 스택 오버플로우 데이터세트는 이미 훈련과 테스트가 나뉘어져 있으므로, 훈련 데이터세트에서 8:2 비율로 검증 세트를 하나 만들겠습니다. `validation_split`을 이용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJOglIaPEoWq",
        "outputId": "e7b5e5f8-27c5-4b47-a21e-1bd42f57d9e4"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = preprocessing.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size = batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 6400 files for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S_xnClnHoAk"
      },
      "source": [
        "위에서 보았듯, 8,000개의 샘플들이 훈련 폴더 내에 있고 그 중 80%인 6,400개가 훈련 데이터세트가 된 것을 확인할 수 있습니다. 6,400개의 데이터는 각각 32개씩 배치(데이터 묶음)을 이루고 있습니다.\n",
        "\n",
        "훈련 데이터세트는 이후에 `tf.data.Dataset`을 거쳐 `model.fit`으로 들어가 학습하는 용도로 쓰이게 됩니다.\n",
        "\n",
        "훈련 데이터세트의 example과 label을 살펴봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoCGfAPGHm8c",
        "outputId": "500ad1f3-119f-496f-95e4-470b5563e33f"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(10):\n",
        "        print(\"Question: \", text_batch.numpy()[i])\n",
        "        print('Label: ', label_batch.numpy()[i])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:  b'\"my tester is going to the wrong constructor i am new to programming so if i ask a question that can be easily fixed, please forgive me. my program has a tester class with a main. when i send that to my regularpolygon class, it sends it to the wrong constructor. i have two constructors. 1 without perameters..public regularpolygon().    {.       mynumsides = 5;.       mysidelength = 30;.    }//end default constructor...and my second, with perameters. ..public regularpolygon(int numsides, double sidelength).    {.        mynumsides = numsides;.        mysidelength = sidelength;.    }// end constructor...in my tester class i have these two lines:..regularpolygon shape = new regularpolygon(numsides, sidelength);.        shape.menu();...numsides and sidelength were declared and initialized earlier in the testing class...so what i want to happen, is the tester class sends numsides and sidelength to the second constructor and use it in that class. but it only uses the default constructor, which therefor ruins the whole rest of the program. can somebody help me?..for those of you who want to see more of my code: here you go..public double vertexangle().    {.        system.out.println(\"\"the vertex angle method: \"\" + mynumsides);// prints out 5.        system.out.println(\"\"the vertex angle method: \"\" + mysidelength); // prints out 30..        double vertexangle;.        vertexangle = ((mynumsides - 2.0) / mynumsides) * 180.0;.        return vertexangle;.    }//end method vertexangle..public void menu().{.    system.out.println(mynumsides); // prints out what the user puts in.    system.out.println(mysidelength); // prints out what the user puts in.    gotographic();.    calcr(mynumsides, mysidelength);.    calcr(mynumsides, mysidelength);.    print(); .}// end menu...this is my entire tester class:..public static void main(string[] arg).{.    int numsides;.    double sidelength;.    scanner keyboard = new scanner(system.in);..    system.out.println(\"\"welcome to the regular polygon program!\"\");.    system.out.println();..    system.out.print(\"\"enter the number of sides of the polygon ==&gt; \"\");.    numsides = keyboard.nextint();.    system.out.println();..    system.out.print(\"\"enter the side length of each side ==&gt; \"\");.    sidelength = keyboard.nextdouble();.    system.out.println();..    regularpolygon shape = new regularpolygon(numsides, sidelength);.    shape.menu();.}//end main...for testing it i sent it numsides 4 and sidelength 100.\"\\n'\n",
            "Label:  1\n",
            "Question:  b'\"blank code slow skin detection this code changes the color space to lab and using a threshold finds the skin area of an image. but it\\'s ridiculously slow. i don\\'t know how to make it faster ?    ..from colormath.color_objects import *..def skindetection(img, treshold=80, color=[255,20,147]):..    print img.shape.    res=img.copy().    for x in range(img.shape[0]):.        for y in range(img.shape[1]):.            rgbimg=rgbcolor(img[x,y,0],img[x,y,1],img[x,y,2]).            labimg=rgbimg.convert_to(\\'lab\\', debug=false).            if (labimg.lab_l &gt; treshold):.                res[x,y,:]=color.            else: .                res[x,y,:]=img[x,y,:]..    return res\"\\n'\n",
            "Label:  3\n",
            "Question:  b'\"option and validation in blank i want to add a new option on my system where i want to add two text files, both rental.txt and customer.txt. inside each text are id numbers of the customer, the videotape they need and the price...i want to place it as an option on my code. right now i have:...add customer.rent return.view list.search.exit...i want to add this as my sixth option. say for example i ordered a video, it would display the price and would let me confirm the price and if i am going to buy it or not...here is my current code:..  import blank.io.*;.    import blank.util.arraylist;.    import static blank.lang.system.out;..    public class rentalsystem{.    static bufferedreader input = new bufferedreader(new inputstreamreader(system.in));.    static file file = new file(\"\"file.txt\"\");.    static arraylist&lt;string&gt; list = new arraylist&lt;string&gt;();.    static int rows;..    public static void main(string[] args) throws exception{.        introduction();.        system.out.print(\"\"nn\"\");.        login();.        system.out.print(\"\"nnnnnnnnnnnnnnnnnnnnnn\"\");.        introduction();.        string repeat;.        do{.            loadfile();.            system.out.print(\"\"nwhat do you want to do?nn\"\");.            system.out.print(\"\"n                    - - - - - - - - - - - - - - - - - - - - - - -\"\");.            system.out.print(\"\"nn                    |     1. add customer    |   2. rent return |n\"\");.            system.out.print(\"\"n                    - - - - - - - - - - - - - - - - - - - - - - -\"\");.            system.out.print(\"\"nn                    |     3. view list       |   4. search      |n\"\");.            system.out.print(\"\"n                    - - - - - - - - - - - - - - - - - - - - - - -\"\");.            system.out.print(\"\"nn                                             |   5. exit        |n\"\");.            system.out.print(\"\"n                                              - - - - - - - - - -\"\");.            system.out.print(\"\"nnchoice:\"\");.            int choice = integer.parseint(input.readline());.            switch(choice){.                case 1:.                    writedata();.                    break;.                case 2:.                    rentdata();.                    break;.                case 3:.                    viewlist();.                    break;.                case 4:.                    search();.                    break;.                case 5:.                    system.out.println(\"\"goodbye!\"\");.                    system.exit(0);.                default:.                    system.out.print(\"\"invalid choice: \"\");.                    break;.            }.            system.out.print(\"\"ndo another task? [y/n] \"\");.            repeat = input.readline();.        }while(repeat.equals(\"\"y\"\"));..        if(repeat!=\"\"y\"\") system.out.println(\"\"ngoodbye!\"\");..    }..    public static void writedata() throws exception{.        system.out.print(\"\"nname: \"\");.        string cname = input.readline();.        system.out.print(\"\"address: \"\");.        string add = input.readline();.        system.out.print(\"\"phone no.: \"\");.        string pno = input.readline();.        system.out.print(\"\"rental amount: \"\");.        string ramount = input.readline();.        system.out.print(\"\"tapenumber: \"\");.        string tno = input.readline();.        system.out.print(\"\"title: \"\");.        string title = input.readline();.        system.out.print(\"\"date borrowed: \"\");.        string dborrowed = input.readline();.        system.out.print(\"\"due date: \"\");.        string ddate = input.readline();.        createline(cname, add, pno, ramount,tno, title, dborrowed, ddate);.        rentdata();.    }..    public static void createline(string name, string address, string phone , string rental, string tapenumber, string title, string borrowed, string due) throws exception{.        filewriter fw = new filewriter(file, true);.        fw.write(\"\"nname: \"\"+name + \"\"naddress: \"\" + address +\"\"nphone no.: \"\"+ phone+\"\"nrentalamount: \"\"+rental+\"\"ntape no.: \"\"+ tapenumber+\"\"ntitle: \"\"+ title+\"\"ndate borrowed: \"\"+borrowed +\"\"ndue date: \"\"+ due+\"\":rn\"\");.        fw.close();.    }..    public static void loadfile() throws exception{.        try{.            list.clear();.            fileinputstream fstream = new fileinputstream(file);.            bufferedreader br = new bufferedreader(new inputstreamreader(fstream));.            rows = 0;.            while( br.ready()).            {.                list.add(br.readline());.                rows++;.            }.            br.close();.        } catch(exception e){.            system.out.println(\"\"list not yet loaded.\"\");.        }.    }..    public static void viewlist(){.        system.out.print(\"\"n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\" |list of all costumers|\"\");.        system.out.print(\"\"~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        for(int i = 0; i &lt;rows; i++){.            system.out.println(list.get(i));.        }.    }.        public static void rentdata()throws exception.    {   system.out.print(\"\"n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\" |rent data list|\"\");.        system.out.print(\"\"~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\"nenter customer name: \"\");.        string cname = input.readline();.        system.out.print(\"\"date borrowed: \"\");.        string dborrowed = input.readline();.        system.out.print(\"\"due date: \"\");.        string ddate = input.readline();.        system.out.print(\"\"return date: \"\");.        string rdate = input.readline();.        system.out.print(\"\"rent amount: \"\");.        string ramount = input.readline();..        system.out.print(\"\"you pay:\"\"+ramount);...    }.    public static void search()throws exception.    {   system.out.print(\"\"n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\" |search costumers|\"\");.        system.out.print(\"\"~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\"\");.        system.out.print(\"\"nenter costumer name: \"\");.        string cname = input.readline();.        boolean found = false;..        for(int i=0; i &lt; rows; i++){.            string temp[] = list.get(i).split(\"\",\"\");..            if(cname.equals(temp[0])){.            system.out.println(\"\"search result:nyou are \"\" + temp[0] + \"\" from \"\" + temp[1] + \"\".\"\"+ temp[2] + \"\".\"\"+ temp[3] + \"\".\"\"+ temp[4] + \"\".\"\"+ temp[5] + \"\" is \"\" + temp[6] + \"\".\"\"+ temp[7] + \"\" is \"\" + temp[8] + \"\".\"\");.                found = true;.            }.        }..        if(!found){.            system.out.print(\"\"no results.\"\");.        }..    }..        public static boolean evaluate(string uname, string pass){.        if (uname.equals(\"\"admin\"\")&amp;&amp;pass.equals(\"\"12345\"\")) return true;.        else return false;.    }..    public static string login()throws exception{.        bufferedreader input=new bufferedreader(new inputstreamreader(system.in));.        int counter=0;.        do{.            system.out.print(\"\"username:\"\");.            string uname =input.readline();.            system.out.print(\"\"password:\"\");.            string pass =input.readline();..            boolean accept= evaluate(uname,pass);..            if(accept){.                break;.                }else{.                    system.out.println(\"\"incorrect username or password!\"\");.                    counter ++;.                    }.        }while(counter&lt;3);..            if(counter !=3) return \"\"login successful\"\";.            else return \"\"login failed\"\";.            }.        public static void introduction() throws exception{..        system.out.println(\"\"                  - - - - - - - - - - - - - - - - - - - - - - - - -\"\");.        system.out.println(\"\"                  !                  r e n t a l                  !\"\");.        system.out.println(\"\"                   ! ~ ~ ~ ~ ~ !  =================  ! ~ ~ ~ ~ ~ !\"\");.        system.out.println(\"\"                  !                  s y s t e m                  !\"\");.        system.out.println(\"\"                  - - - - - - - - - - - - - - - - - - - - - - - - -\"\");.        }..}\"\\n'\n",
            "Label:  1\n",
            "Question:  b'\"exception: dynamic sql generation for the updatecommand is not supported against a selectcommand that does not return any key i dont know what is the problem this my code : ..string nomtable;..datatable listeetablissementtable = new datatable();.datatable listeinteretstable = new datatable();.dataset ds = new dataset();.sqldataadapter da;.sqlcommandbuilder cmdb;..private void listeinterets_click(object sender, eventargs e).{.    nomtable = \"\"listeinteretstable\"\";.    d.cnx.open();.    da = new sqldataadapter(\"\"select nome from offices\"\", d.cnx);.    ds = new dataset();.    da.fill(ds, nomtable);.    datagridview1.datasource = ds.tables[nomtable];.}..private void sauvgarder_click(object sender, eventargs e).{.    d.cnx.open();.    cmdb = new sqlcommandbuilder(da);.    da.update(ds, nomtable);.    d.cnx.close();.}\"\\n'\n",
            "Label:  0\n",
            "Question:  b'\"parameter with question mark and super in blank, i\\'ve come across a method that is formatted like this:..public final subscription subscribe(final action1&lt;? super t&gt; onnext, final action1&lt;throwable&gt; onerror) {.}...in the first parameter, what does the question mark and super mean?\"\\n'\n",
            "Label:  1\n",
            "Question:  b'call two objects wsdl the first time i got a very strange wsdl. ..i would like to call the object (interface - invoicecheck_out) do you know how?....i would like to call the object (variable) do you know how?..try to call (it`s ok)....try to call (how call this?)\\n'\n",
            "Label:  0\n",
            "Question:  b\"how to correctly make the icon for systemtray in blank using icon sizes of any dimension for systemtray doesn't look good overall. .what is the correct way of making icons for windows system tray?..screenshots: http://imgur.com/zsibwn9..icon: http://imgur.com/vsh4zo8\\n\"\n",
            "Label:  0\n",
            "Question:  b'\"is there a way to check a variable that exists in a different script than the original one? i\\'m trying to check if a variable, which was previously set to true in 2.py in 1.py, as 1.py is only supposed to continue if the variable is true...2.py..import os..completed = false..#some stuff here..completed = true...1.py..import 2 ..if completed == true.   #do things...however i get a syntax error at ..if completed == true\"\\n'\n",
            "Label:  3\n",
            "Question:  b'\"blank control flow i made a number which asks for 2 numbers with blank and responds with  the corresponding message for the case. how come it doesnt work  for the second number ? .regardless what i enter for the second number , i am getting the message \"\"your number is in the range 0-10\"\"...using system;.using system.collections.generic;.using system.linq;.using system.text;..namespace consoleapplication1.{.    class program.    {.        static void main(string[] args).        {.            string myinput;  // declaring the type of the variables.            int myint;..            string number1;.            int number;...            console.writeline(\"\"enter a number\"\");.            myinput = console.readline(); //muyinput is a string  which is entry input.            myint = int32.parse(myinput); // myint converts the string into an integer..            if (myint &gt; 0).                console.writeline(\"\"your number {0} is greater than zero.\"\", myint);.            else if (myint &lt; 0).                console.writeline(\"\"your number {0} is  less  than zero.\"\", myint);.            else.                console.writeline(\"\"your number {0} is equal zero.\"\", myint);..            console.writeline(\"\"enter another number\"\");.            number1 = console.readline(); .            number = int32.parse(myinput); ..            if (number &lt; 0 || number == 0).                console.writeline(\"\"your number {0} is  less  than zero or equal zero.\"\", number);.            else if (number &gt; 0 &amp;&amp; number &lt;= 10).                console.writeline(\"\"your number {0} is  in the range from 0 to 10.\"\", number);.            else.                console.writeline(\"\"your number {0} is greater than 10.\"\", number);..            console.writeline(\"\"enter another number\"\");..        }.    }    .}\"\\n'\n",
            "Label:  0\n",
            "Question:  b'\"credentials cannot be used for ntlm authentication i am getting org.apache.commons.httpclient.auth.invalidcredentialsexception: credentials cannot be used for ntlm authentication: exception in eclipse..whether it is possible mention eclipse to take system proxy settings directly?..public class httpgetproxy {.    private static final string proxy_host = \"\"proxy.****.com\"\";.    private static final int proxy_port = 6050;..    public static void main(string[] args) {.        httpclient client = new httpclient();.        httpmethod method = new getmethod(\"\"https://kodeblank.org\"\");..        hostconfiguration config = client.gethostconfiguration();.        config.setproxy(proxy_host, proxy_port);..        string username = \"\"*****\"\";.        string password = \"\"*****\"\";.        credentials credentials = new usernamepasswordcredentials(username, password);.        authscope authscope = new authscope(proxy_host, proxy_port);..        client.getstate().setproxycredentials(authscope, credentials);..        try {.            client.executemethod(method);..            if (method.getstatuscode() == httpstatus.sc_ok) {.                string response = method.getresponsebodyasstring();.                system.out.println(\"\"response = \"\" + response);.            }.        } catch (ioexception e) {.            e.printstacktrace();.        } finally {.            method.releaseconnection();.        }.    }.}...exception:...  dec 08, 2017 1:41:39 pm .          org.apache.commons.httpclient.auth.authchallengeprocessor selectauthscheme.         info: ntlm authentication scheme selected.       dec 08, 2017 1:41:39 pm org.apache.commons.httpclient.httpmethoddirector executeconnect.         severe: credentials cannot be used for ntlm authentication: .           org.apache.commons.httpclient.usernamepasswordcredentials.           org.apache.commons.httpclient.auth.invalidcredentialsexception: credentials .         cannot be used for ntlm authentication: .        enter code here .          org.apache.commons.httpclient.usernamepasswordcredentials.      at org.apache.commons.httpclient.auth.ntlmscheme.authenticate(ntlmscheme.blank:332).        at org.apache.commons.httpclient.httpmethoddirector.authenticateproxy(httpmethoddirector.blank:320).      at org.apache.commons.httpclient.httpmethoddirector.executeconnect(httpmethoddirector.blank:491).      at org.apache.commons.httpclient.httpmethoddirector.executewithretry(httpmethoddirector.blank:391).      at org.apache.commons.httpclient.httpmethoddirector.executemethod(httpmethoddirector.blank:171).      at org.apache.commons.httpclient.httpclient.executemethod(httpclient.blank:397).      at org.apache.commons.httpclient.httpclient.executemethod(httpclient.blank:323).      at httpgetproxy.main(httpgetproxy.blank:31).  dec 08, 2017 1:41:39 pm org.apache.commons.httpclient.httpmethoddirector processproxyauthchallenge.  info: failure authenticating with ntlm @proxy.****.com:6050\"\\n'\n",
            "Label:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5X4Ux6bIbs4"
      },
      "source": [
        "첫 번째 배치의 32개의 데이터 중 10개의 데이터를 출력해보았습니다. \n",
        "\n",
        "레이블이 0, 1, 2, 3으로 나타나는데 각 레이블이 의미하는 언어를 알아보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "engcVqXJIX6f",
        "outputId": "de50a603-dd42-4c5d-bceb-5e17ffc041b8"
      },
      "source": [
        "for i, label in enumerate(raw_train_ds.class_names):\n",
        "    print(\"Label\", i, \"corresponds to\", label)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label 0 corresponds to csharp\n",
            "Label 1 corresponds to java\n",
            "Label 2 corresponds to javascript\n",
            "Label 3 corresponds to python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a856jjvAJDjA"
      },
      "source": [
        "이제 검증 데이터세트를 만들어보겠습니다.\n",
        "\n",
        "주의: `validation_split`과 `subset`을 사용할 때 반드시 랜덤 seed를 명시하거나 `shuffle=False`을 적어주어, 훈련 데이터세트와 검증 데이터세트가 서로 겹치지 않도록 해야합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOmSAk_jI-cc",
        "outputId": "795eaed5-0484-4874-9943-d17d10108e8f"
      },
      "source": [
        "raw_val_ds = preprocessing.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size = batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n",
            "Using 1600 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhR79GOfJ3Kj"
      },
      "source": [
        "마지막으로 훈련 데이터세트를 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfw02g68JpR-",
        "outputId": "b61b754f-e25f-431a-8f65-49be2c5fae4d"
      },
      "source": [
        "test_dir = dataset_dir/'test'\n",
        "raw_test_ds = preprocessing.text_dataset_from_directory(\n",
        "    test_dir,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 files belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-hTKO31J5iK"
      },
      "source": [
        "### 훈련을 위한 데이터세트 준비하기(전처리 작업)\n",
        "\n",
        "이제 `preprocessing.TextVectorization` 레이어에 사용할 데이터를 표준화, 토큰화, 벡터화하는 전처리 작업을 거쳐야합니다.\n",
        "\n",
        "+ 표준화(Standardization) : 구두점 또는 HTML의 요소들을 제거해서 데이터세트를 단순화하는 텍스트 전처리 과정.\n",
        "+ 토큰화(Tokenization) : 문자열을 토큰으로 나누는 과정(예를 들어, 문장을 띄어쓰기에 따라 나눠서 단어로 만드는 과정이 이에 속합니다)\n",
        "+ 벡터화(Vectorization) : 나누어진 토큰들을 숫자로 변환하는 과정입니다. 벡터화된 숫자들이 신경망에 input으로 입력됩니다.\n",
        "\n",
        "각 과정들은 아래와 같이 레이어됩니다.\n",
        "\n",
        "+ 디폴트 표준화 작업은 텍스트를 소문자로 변환하고 구두점을 제거합니다.\n",
        "+ 디폴트 토큰화 작업은 공백에 따라 문장을 분리합니다.\n",
        "+ 디폴트 벡터화 모드는 `int`입니다. 각 정수는 하나의 단어를 가리킵니다. `binary`라는 다른 모드를 사용할 수도 있습니다. 이 모드는 bag-of-word 모델을 빌드하기 위함입니다.   \n",
        "\n",
        "벡터화 모드의 `binary`와 `int` 두 가지 모드를 모두 배워보겠습니다. 먼저 `binary` 모드를 이용해 bag-of-words 모델을 빌드해본 뒤에 `int` 모드를 `1D ConvNet`과 함께 사용해보겠습니다.\n",
        "\n",
        "먼저 각 `binary`와 `int` 벡터화 레이어를 만든 뒤에 `adapt`를 이용해 훈련 될 수 있도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMfC6Me-JyQx"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "# binary layer\n",
        "binary_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='binary'\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qg47fNbS9ZT"
      },
      "source": [
        "`int` 모드에서는 maximum vocabulary size와 더불어 maximum sequence length도 지정해주어야 합니다. maximum length를 벗어나는 문장의 경우 앞 또는 뒤에서부터 문장을 자릅니다. maximum length보다 부족한 길이일 경우 부족한만큼 0으로 채웁니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_p65CvnUIm3"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# int layer\n",
        "int_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KckhEPfpUbGw"
      },
      "source": [
        "다음으로 `adapt`를 콜해서 전처리 레이어의 상태를 데이터세트를 이용해 훈련시켜야합니다. 이 과정은 모델이 문자에 해당하는 정수를 빌드하도록 즉, 벡터화 해줍니다.\n",
        "\n",
        "주의: `adapt`를 콜할 때 반드시 훈련 데이터만을 사용해야합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx2jFLDTUTnQ"
      },
      "source": [
        "train_text = raw_train_ds.map(lambda text, label: text)\n",
        "binary_vectorize_layer.adapt(train_text)\n",
        "int_vectorize_layer.adapt(train_text)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6QDo5poWp1Z"
      },
      "source": [
        "데이터를 전처리하기 위해 이 레이어들을 이용한 결과를 살펴봅시다.\n",
        "\n",
        "[tf.expand_dims](https://www.tensorflow.org/api_docs/python/tf/expand_dims)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC34WTudW1tJ"
      },
      "source": [
        "def binary_vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)  # text의 마지막에 1을 추가한다.\n",
        "    return binary_vectorize_layer(text), label"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnsfBLx4XTSY"
      },
      "source": [
        "def int_vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return int_vectorize_layer(text), label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Bv8ZgDXaXT",
        "outputId": "e5ecb34b-fd6a-4fb5-fc1d-b167e1a5f8e8"
      },
      "source": [
        "# 하나의 배치(32개의 데이터)를 데이터세트로부터 뽑아냅니다.\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_question, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Question: \", first_question)\n",
        "print('Label: ', first_label)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:  tf.Tensor(b'\"what is the difference between these two ways to create an element? var a = document.createelement(\\'div\\');..a.id = \"\"mydiv\"\";...and..var a = document.createelement(\\'div\\').id = \"\"mydiv\"\";...what is the difference between them such that the first one works and the second one doesn\\'t?\"\\n', shape=(), dtype=string)\n",
            "Label:  tf.Tensor(2, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAKWKzM-XuC-",
        "outputId": "18fc63ee-0fc2-4dd8-9bb5-08c9262390ba"
      },
      "source": [
        "print(\"'binary' vectorized question:\", binary_vectorize_text(first_question, first_label)[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'binary' vectorized question: tf.Tensor([[1. 1. 0. ... 0. 0. 0.]], shape=(1, 10000), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFDhB7I3ZaDX",
        "outputId": "ca1638b0-8d23-4b78-8519-2bd439ae0d22"
      },
      "source": [
        "print(\"'int' vectorized question:\", int_vectorize_text(first_question, first_label)[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'int' vectorized question: tf.Tensor(\n",
            "[[ 55   6   2 410 211 229 121 895   4 124  32 245  43   5   1   1   5   1\n",
            "    1   6   2 410 211 191 318  14   2  98  71 188   8   2 199  71 178   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]], shape=(1, 250), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0-kkw09Zgfg"
      },
      "source": [
        "위에서 보았듯이 `binary` 모드는 maximum vocabulary size내에 속해 있는지 여부를 0과 1로 나타내는 반면, `int` 모드는 각 토큰을 해당하는 정수로 변환해서 질서를 보존합니다.\n",
        "\n",
        "`.get_vocabulary()` 메서드를 통해 정수에 대응되는 문자가 무엇인지 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlfEEq9iZfzL",
        "outputId": "131052fb-0fa3-44e9-8bc7-e39a08f83464"
      },
      "source": [
        "print('1289 ---> ', int_vectorize_layer.get_vocabulary()[1289])\n",
        "print('313 ---> ', int_vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(int_vectorize_layer.get_vocabulary())))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1289 --->  roman\n",
            "313 --->  source\n",
            "Vocabulary size: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFXPyjFccOLp"
      },
      "source": [
        "이제 모델을 훈련시킬 준비가 거의 끝났습니다. 마지막 전처리 과정으로 `TextVectorization` 레이어를 훈련, 검증, 테스트 데이터세트에 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUlwSx8CbklL"
      },
      "source": [
        "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
        "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
        "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
        "\n",
        "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
        "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
        "int_test_ds = raw_test_ds.map(int_vectorize_text)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pls_RMcddaq"
      },
      "source": [
        "### 훈련 성능 향상시키기\n",
        "\n",
        "I/O가 블로킹되지 않도록 데이터를 로딩할 때 필수적으로 해야할 2가지가 있습니다.\n",
        "\n",
        "+ `.cache()` : 디스크로부터 데이터를 로드한 후에 메모리에 남겨놓도록 합니다. 모델을 훈련할 때 데이터세트가 소위 병목(병의 목 부분처럼 좁아서 흐름이 지연되는 현상) 현상이 되는 것을 방지할 수 있습니다. 데이터세트의 크기가 너무 커서 메모리 안에 훈련될 수 없다면, 이 방법이 작은 파일들을 읽는 것보다 훨씬 효율적으로 데이터를 읽어들일 수 있는 캐시를 생성해서 해결할 수 있습니다.\n",
        "\n",
        "+ `.prefetch()` : 훈련하는 동안 데이터의 전처리 과정과 모델 학습 작용이 동시에 이루어지도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt2TxpbbfffT"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE  # 컴퓨터의 CPU에 맞는 적절한 구동 코어수 결정\n",
        "# 데이터 세트를 캐시 및 프리패치하는 함수 생성\n",
        "def configure_dataset(dataset):\n",
        "    return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWhwKoH5fqSE"
      },
      "source": [
        "binary_train_ds = configure_dataset(binary_train_ds)\n",
        "binary_val_ds = configure_dataset(binary_val_ds)\n",
        "binary_test_ds = configure_dataset(binary_test_ds)\n",
        "\n",
        "int_train_ds = configure_dataset(int_train_ds)\n",
        "int_val_ds = configure_dataset(int_val_ds)\n",
        "int_test_ds = configure_dataset(int_test_ds)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rnCNv7GgEUJ"
      },
      "source": [
        "### 모델 훈련하기\n",
        "\n",
        "이제 신경망을 구성할 차례입니다.\n",
        "\n",
        "`binary` 벡터화 데이터를 이용해 간단한 bag-of-words 선형 모델을 학습해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RLAuspBf83h",
        "outputId": "671c3188-6bea-4741-cd0a-d9e1c6a3c634"
      },
      "source": [
        "binary_model = tf.keras.Sequential([layers.Dense(4)])\n",
        "binary_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history = binary_model.fit(\n",
        "    binary_train_ds, \n",
        "    validation_data=binary_val_ds, \n",
        "    epochs=10\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "200/200 [==============================] - 6s 26ms/step - loss: 1.1243 - accuracy: 0.6341 - val_loss: 0.9192 - val_accuracy: 0.7744\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.7821 - accuracy: 0.8183 - val_loss: 0.7535 - val_accuracy: 0.7975\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.8578 - val_loss: 0.6670 - val_accuracy: 0.8144\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.5359 - accuracy: 0.8866 - val_loss: 0.6130 - val_accuracy: 0.8206\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.4696 - accuracy: 0.9027 - val_loss: 0.5760 - val_accuracy: 0.8294\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.4191 - accuracy: 0.9162 - val_loss: 0.5492 - val_accuracy: 0.8344\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.3787 - accuracy: 0.9277 - val_loss: 0.5289 - val_accuracy: 0.8356\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.3453 - accuracy: 0.9369 - val_loss: 0.5132 - val_accuracy: 0.8344\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.3170 - accuracy: 0.9416 - val_loss: 0.5008 - val_accuracy: 0.8344\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 1s 4ms/step - loss: 0.2926 - accuracy: 0.9480 - val_loss: 0.4910 - val_accuracy: 0.8394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uylTxtjKg7ud"
      },
      "source": [
        "이제 `int` 벡터화 레이어를 사용해 `1D ConvNet`을 빌드해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW86kv4UgsqG"
      },
      "source": [
        "def create_model(vocab_size, num_labels):\n",
        "    model = tf.keras.Sequential([\n",
        "                                 layers.Embedding(vocab_size, 64, mask_zero=True),\n",
        "                                 layers.Conv1D(64, 5, padding='valid', activation='relu', strides=2),\n",
        "                                 layers.GlobalMaxPooling1D(),\n",
        "                                 layers.Dense(num_labels)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-j0lNH4iV81",
        "outputId": "334b09f0-934d-49a0-b82d-c05508bc9ae7"
      },
      "source": [
        "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4)\n",
        "int_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "200/200 [==============================] - 8s 38ms/step - loss: 1.1352 - accuracy: 0.5078 - val_loss: 0.7420 - val_accuracy: 0.7038\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 5s 27ms/step - loss: 0.6174 - accuracy: 0.7616 - val_loss: 0.5485 - val_accuracy: 0.8000\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 10s 52ms/step - loss: 0.3762 - accuracy: 0.8784 - val_loss: 0.4952 - val_accuracy: 0.8112\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 6s 28ms/step - loss: 0.2123 - accuracy: 0.9495 - val_loss: 0.4998 - val_accuracy: 0.8156\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 6s 28ms/step - loss: 0.1081 - accuracy: 0.9812 - val_loss: 0.5287 - val_accuracy: 0.8150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBjANO7elY_v"
      },
      "source": [
        "두 모델을 비교해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yceMNnWoirpu",
        "outputId": "4679a6b9-e9e8-4f72-c7cd-549bd8715e5c"
      },
      "source": [
        "print('Linear model on binary vectorized data:')\n",
        "print(binary_model.summary())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear model on binary vectorized data:\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 4)                 40004     \n",
            "=================================================================\n",
            "Total params: 40,004\n",
            "Trainable params: 40,004\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efbuUch6ljRC",
        "outputId": "7b990b39-c246-46e8-f4f8-9047af41fe7a"
      },
      "source": [
        "print(\"ConvNet model on int vectorized data:\")\n",
        "print(int_model.summary())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet model on int vectorized data:\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          640064    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 64)          20544     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Globa  (None, 64)                0         \n",
            "lMaxPooling1D)                                                   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 260       \n",
            "=================================================================\n",
            "Total params: 660,868\n",
            "Trainable params: 660,868\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpAtWT_nltnQ"
      },
      "source": [
        "두 모델을 테스트 데이터를 이용해 평가해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YupX7-wvlsU2",
        "outputId": "cf5b7a89-e952-4030-80e7-92ecab651aaf"
      },
      "source": [
        "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n",
        "int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n",
        "\n",
        "print(\"Binary model accuracy: {:2.2%}\".format(binary_accuracy))\n",
        "print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 3s 10ms/step - loss: 0.5183 - accuracy: 0.8141\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 0.5367 - accuracy: 0.8091\n",
            "Binary model accuracy: 81.41%\n",
            "Int model accuracy: 80.91%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FutdOCOCm9hy"
      },
      "source": [
        "주의: 이 데이터세트는 다소 간단한 분류 문제를 보여줍니다. 더 복잡한 데이터세트와 문제들은 전처리 전략과 모델 아키텍처에 따라서 사소하지만 중요한 변화를 가져옵니다. 여러 다른 하이퍼파라미터와 에포크를 시도해보면서 다양한 접근을 해보는 것이 중요합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egG_nGp_nmXi"
      },
      "source": [
        "### 모델 익스포트하기\n",
        "\n",
        "앞서 텍스트를 모델에 입력하기 전에 `TextVectorization` 레이어를 적용했습니다. 만약 전처리 되지 않은 문자열을 바로 모델에 입력하고 싶다면, `TextVectorization` 레이어를 모델 안에 넣으면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRBfowY7mwQg",
        "outputId": "fe4af9e1-1701-46b4-faf7-8f8b56df4ec6"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "                                    binary_vectorize_layer,\n",
        "                                    binary_model,\n",
        "                                    layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:4908: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 3s 12ms/step - loss: 0.5183 - accuracy: 0.8141\n",
            "Accuracy: 81.41%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBHL6EsOpE4C"
      },
      "source": [
        "이미 `binary_vectorize_layer`와 `binary_model`은 앞서 훈련이 되었으므로 새로 만들어진 `export_model`을 따로 훈련하진 않았습니다.\n",
        "\n",
        "이렇게 함으로써 모델에 전처리 되지 않은 문자열을 입력으로 넣고 `model.predict`를 사용해 각 레이블에 대한 확률을 예측할 수 있습니다.\n",
        "\n",
        "최댓값을 가지는 레이블을 찾는 함수를 정의해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e54iLqcoym7"
      },
      "source": [
        "def get_string_labels(predicted_scores_batch):\n",
        "    predicted_int_labels = tf.argmax(predicted_scores_batch, axis=1)\n",
        "    predicted_labels = tf.gather(raw_train_ds.class_names, predicted_int_labels)\n",
        "    return predicted_labels"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMNJUJnSqQP7"
      },
      "source": [
        "### 새로운 데이터를 사용해 결과 예측하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ykzpbupqO7f",
        "outputId": "36dbfd20-a9f1-4cc9-ff1f-25e233bf2aaa"
      },
      "source": [
        "inputs = [\n",
        "          \"how do I extract keys from a dict into a list?\",  # python\n",
        "          \"debug public static void main(string[] args) {...}\",  # java\n",
        "]\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = get_string_labels(predicted_scores)\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "    print(\"Question: \", input)\n",
        "    print(\"Predicted label: \", label.numpy())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:  how do I extract keys from a dict into a list?\n",
            "Predicted label:  b'python'\n",
            "Question:  debug public static void main(string[] args) {...}\n",
            "Predicted label:  b'java'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unTvPBdcrRik"
      },
      "source": [
        "텍스트 전처리 로직을 모델 안에 포함하는 것은 새로운 데이터를 예측할 때 훨씬 단순화하고 잠재적인 [train/test skew](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew)를 감소시켜줍니다.\n",
        "\n",
        "`TextVectorization` 레이어를 어디에 적용시키느냐에 따라 성능에 차이가 생긴다는 점을 명심해야합니다. 이 레이어를 모델 밖에서 사용하면 CPU 처리와 GPU에서의 훈련 데이터 버퍼링이 동시에 일어나지 않습니다. 그래서 GPU에서 모델을 훈련한다면, 모델의 좋은 훈련 성능을 위한 선택이 될 수 있습니다. 그리고 나서 `TextVectorization` 레이어를 모델 안으로 위치를 바꿔주면 앞으로 새로운 데이터를 예측할 때 좋은 성능을 낼 수 있을 것입니다."
      ]
    }
  ]
}