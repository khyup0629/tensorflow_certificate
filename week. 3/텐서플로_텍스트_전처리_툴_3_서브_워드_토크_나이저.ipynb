{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "텐서플로 텍스트 전처리 툴 #3-서브 워드 토크 나이저",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ruo3sH6Wlwk"
      },
      "source": [
        "# 서브 워드 토크 나이저\n",
        "\n",
        "이번 튜토리얼에서는 `text.BertTokenizer`를 이용해 데이터 세트에서 subword 어휘를 생성하고 구축하는 방법을 보여줍니다.\n",
        "\n",
        "`tensorflow_text` 패키지에는 토크나이저들이 구현되어 있습니다. 그 중 3가지는 아래와 같습니다.\n",
        "\n",
        "+ `text_BertTokenizer` : 높은 수준의 인터페이스를 갖고 있습니다. BERT의 토큰 분할 알고리즘과 `WordPieceTokenizer`가 포함되어 있습니다. 문장을 입력받아 토큰 ID를 반환합니다.\n",
        "+ `text.WordpieceTokenizer` : 낮은 수준의 인터페이스를 갖습니다. WordPiece 알고리즘만을 구현했습니다. 텍스트를 호출하기 전에 표준화 작업과 단어를 분할하는 작업이 선행되어야 합니다. 단어를 입력받아 토큰 ID를 반환합니다.\n",
        "+ `text.SentencepieceTokenizer` : 사전 훈련된 문장 모델이 필요합니다. 이 모델을 이용해서 문장을 입력으로 받을 수 있습니다.\n",
        "\n",
        "본 튜토리얼은 하향식 방식으로 Wordpiece 어휘를 빌드합니다. 일본어, 중국어, 한국어에는 동작하지 않는 프로세스입니다. 이러한 언어를 토큰화하려면 `text.SentencepieceTokenizer` 또는 [text.UnicodeCharTokenizer](https://www.tensorflow.org/text/api_docs/python/text/UnicodeCharTokenizer)를 이용해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVFlEXeXWc8U"
      },
      "source": [
        "!pip install -q -U tensorflow-text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GGAErBbdiZW"
      },
      "source": [
        "!pip install -q tensorflow_datasets"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYgAfaNLdl2T"
      },
      "source": [
        "import collections\n",
        "import os, pathlib, re, string, sys, tempfile, time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bWGA5Tqd2A5"
      },
      "source": [
        "tf.get_logger().setLevel('ERROR')\n",
        "pwd = pathlib.Path.cwd()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7dakPkMeCpc"
      },
      "source": [
        "## 데이터셋 다운로드\n",
        "\n",
        "`tensorflow_datasets`에서 포르투갈어 / 영어 번역 데이터셋을 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKHnGqHQeNwE"
      },
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmdOgHrMeXoL"
      },
      "source": [
        "이 데이터셋은 포르투갈어 / 영어 문장 쌍을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPhGttWzeod8",
        "outputId": "bdbc51de-2965-42cb-c087-996b941dae28"
      },
      "source": [
        "for pt, en in train_examples.take(1):\n",
        "    print(\"Portuguese: \", pt.numpy().decode('utf-8'))\n",
        "    print(\"English: \", pt.numpy().decode('utf-8'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Portuguese:  e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "English:  e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBZUxD35exwK"
      },
      "source": [
        "데이터셋은 아래와 같은 사항에 유의해야 합니다.\n",
        "\n",
        "+ 소문자로 이루어져있습니다.\n",
        "+ 구두점 주위에 공백이 있습니다.\n",
        "+ 그렇다고 이 데이터셋의 정규화 여부가 명확하지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO8HCo2Jfbpd"
      },
      "source": [
        "train_en = train_examples.map(lambda pt, en: en)\n",
        "train_pt = train_examples.map(lambda pt, en: pt)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP9Hos1tfiXd"
      },
      "source": [
        "## 어휘 생성\n",
        "\n",
        "이번 섹션에서는 데이터셋에서 vocabulary를 생성합니다. vocabulary 파일은 이미 있고 생성 코드가 `tensorflow_text` pip 패키지에 포함되어 있습니다. 자동으로 가져 오지 않기 때문에 수동으로 가져와야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_vqqi6xgBnh"
      },
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeF1zOO-gIeJ"
      },
      "source": [
        "`bert_vocab.bert_vocab_from_dataset` 함수는 vocabulary를 생성합니다.\n",
        "\n",
        "[Wordpiece 알고리즘](#algorithm) 설명\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGUs6wxEkv5p"
      },
      "source": [
        "bert_tokenizer_params = dict(lower_case=True)  # 정규화(소문자화)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "\n",
        "bert_vocab_args = dict(\n",
        "    vocab_size = 8000,\n",
        "    reserved_tokens=reserved_tokens,\n",
        "    bert_tokenizer_params=bert_tokenizer_params,\n",
        "    learn_params={}\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7Yky14tk2hX",
        "outputId": "a732bccc-a855-4ed9-bd2a-03985625e9d1"
      },
      "source": [
        "%%time\n",
        "pt_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_pt.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 18s, sys: 3.12 s, total: 2min 21s\n",
            "Wall time: 2min 19s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJRtVN2cmfuR",
        "outputId": "d219efb5-8555-4733-cf3d-a21a2daf407e"
      },
      "source": [
        "print(pt_vocab[:10])\n",
        "print(pt_vocab[100:110])\n",
        "print(pt_vocab[1000:1010])\n",
        "print(pt_vocab[-10:])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
            "['no', 'por', 'mais', 'na', 'eu', 'esta', 'muito', 'isso', 'isto', 'sao']\n",
            "['90', 'desse', 'efeito', 'malaria', 'normalmente', 'palestra', 'recentemente', '##nca', 'bons', 'chave']\n",
            "['##–', '##—', '##‘', '##’', '##“', '##”', '##⁄', '##€', '##♪', '##♫']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6ABtwJmpXAZ"
      },
      "source": [
        "어휘 파일을 작성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR01eMlpot26"
      },
      "source": [
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CNwS-fPoyRx"
      },
      "source": [
        "write_vocab_file('pt_vocab.txt', pt_vocab)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggThuS4vpFGD",
        "outputId": "d96ff590-e22a-4145-e6fd-86cc64a7870d"
      },
      "source": [
        "%%time\n",
        "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    train_en.batch(1000).prefetch(2),\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 35s, sys: 2.98 s, total: 1min 38s\n",
            "Wall time: 1min 36s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}